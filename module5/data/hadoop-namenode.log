2025-04-13 05:54:31 INFO  NameNode:809 - STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = hadoop-namenode/172.19.0.2
STARTUP_MSG:   args = [-format, -force, ]
STARTUP_MSG:   version = 3.4.1
STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/netty-common-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/kerby-asn1-2.0.3.jar:/opt/hadoop/share/hadoop/common/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.100.Final-linux-x86_64.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.100.Final-linux-aarch_64.jar:/opt/hadoop/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-security-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-mqtt-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-haproxy-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-http2-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.9.0.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-dns-classes-macos-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-3.4.1.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.9.2.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-http-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/checker-qual-2.5.2.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-classes-kqueue-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-redis-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-xml-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.55.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-dns-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-5.2.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.9.0.jar:/opt/hadoop/share/hadoop/common/lib/kerb-core-2.0.3.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-aarch_64.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration2-2.10.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.26.1.jar:/opt/hadoop/share/hadoop/common/lib/bcprov-jdk18on-1.78.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/common/lib/jetty-http-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/common/lib/kerby-pkix-2.0.3.jar:/opt/hadoop/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/hadoop/common/lib/dnsjava-3.6.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-unix-common-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.19.4.jar:/opt/hadoop/share/hadoop/common/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang3-3.12.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-jute-3.8.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.5.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-servlet-1.19.4.jar:/opt/hadoop/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop/share/hadoop/common/lib/jetty-xml-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/common/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-buffer-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-handler-ssl-ocsp-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/jetty-server-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.19.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.6.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-5.2.0.jar:/opt/hadoop/share/hadoop/common/lib/netty-handler-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.13.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.100.Final-osx-x86_64.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-ajax-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-udt-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-dns-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-sctp-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/kerby-util-2.0.3.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.2.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.8.4.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-smtp-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-5.2.0.jar:/opt/hadoop/share/hadoop/common/lib/failureaccess-1.0.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-classes-epoll-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.22.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.15.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.1.10.4.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-rxtx-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-socks-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.16.1.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/re2j-1.1.jar:/opt/hadoop/share/hadoop/common/lib/kerb-crypto-2.0.3.jar:/opt/hadoop/share/hadoop/common/lib/jetty-servlet-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-2.12.7.jar:/opt/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar:/opt/hadoop/share/hadoop/common/lib/reload4j-1.2.22.jar:/opt/hadoop/share/hadoop/common/lib/jackson-databind-2.12.7.1.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-9.37.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-text-1.10.0.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-3.4.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-memcache-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.5.4.jar:/opt/hadoop/share/hadoop/common/lib/kerby-config-2.0.3.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.13.jar:/opt/hadoop/share/hadoop/common/lib/netty-handler-proxy-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-stomp-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/jul-to-slf4j-1.7.36.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/kerb-util-2.0.3.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-x86_64.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-shaded-protobuf_3_25-1.3.0.jar:/opt/hadoop/share/hadoop/common/lib/audience-annotations-0.12.0.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.2.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.100.Final-osx-aarch_64.jar:/opt/hadoop/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-io-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/common/lib/netty-all-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-shaded-guava-1.3.0.jar:/opt/hadoop/share/hadoop/common/lib/jackson-annotations-2.12.7.jar:/opt/hadoop/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/hadoop/common/lib/jetty-webapp-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-3.4.1.jar:/opt/hadoop/share/hadoop/common/hadoop-common-3.4.1.jar:/opt/hadoop/share/hadoop/common/hadoop-common-3.4.1-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-registry-3.4.1.jar:/opt/hadoop/share/hadoop/common/hadoop-kms-3.4.1.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/netty-common-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-asn1-2.0.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.100.Final-linux-x86_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.100.Final-linux-aarch_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-security-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-mqtt-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-haproxy-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-http2-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/gson-2.9.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-classes-macos-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-auth-3.4.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/avro-1.9.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-http-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-classes-kqueue-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-redis-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-xml-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-dns-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-client-5.2.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-net-3.9.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-core-2.0.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-aarch_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-configuration2-2.10.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-compress-1.26.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-http-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-pkix-2.0.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/hadoop/hdfs/lib/dnsjava-3.6.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-unix-common-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.19.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang3-3.12.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/zookeeper-jute-3.8.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-servlet-1.19.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-xml-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-buffer-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-handler-ssl-ocsp-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-server-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.19.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-math3-3.6.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-recipes-5.2.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/HikariCP-4.0.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-handler-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.100.Final-osx-x86_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-udt-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-sctp-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-util-2.0.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/zookeeper-3.8.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-smtp-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-framework-5.2.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-classes-epoll-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-json-1.22.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.15.jar:/opt/hadoop/share/hadoop/hdfs/lib/snappy-java-1.1.10.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-rxtx-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-socks-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.16.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/hdfs/lib/re2j-1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-crypto-2.0.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-servlet-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-2.12.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/opt/hadoop/share/hadoop/hdfs/lib/reload4j-1.2.22.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-databind-2.12.7.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.37.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-text-1.10.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-annotations-3.4.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-memcache-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jettison-1.5.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-config-2.0.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-handler-proxy-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-stomp-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-util-2.0.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-x86_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_25-1.3.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/audience-annotations-0.12.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.100.Final-osx-aarch_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-io-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-shaded-guava-1.3.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-annotations-2.12.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-webapp-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.4.1.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.4.1.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.4.1-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.4.1-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.4.1-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.4.1-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-3.4.1.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.4.1.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.4.1.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.4.1-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.4.1.jar:/opt/hadoop/share/hadoop/yarn:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.12.7.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-jndi-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/yarn/lib/javax-websocket-server-impl-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-api-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.websocket-client-api-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/cache-api-1.1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-server-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-base-2.12.7.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-annotations-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/yarn/lib/javax-websocket-client-impl-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/yarn/lib/ehcache-3.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/FastInfoset-1.2.15.jar:/opt/hadoop/share/hadoop/yarn/lib/snakeyaml-2.0.jar:/opt/hadoop/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-servlet-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-commons-9.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-4.2.3.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.19.4.jar:/opt/hadoop/share/hadoop/yarn/lib/jna-5.2.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-tree-9.6.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-plus-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/yarn/lib/istack-commons-runtime-3.0.7.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-ex-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/txw2-2.3.1.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-client-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-client-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/yarn/lib/bcutil-jdk18on-1.78.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jline-3.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/codemodel-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.websocket-api-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-4.2.3.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.12.7.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.19.4.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-runtime-2.3.1.jar:/opt/hadoop/share/hadoop/yarn/lib/bcpkix-jdk18on-1.78.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsonschema2pojo-core-1.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/hadoop/yarn/lib/objenesis-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-common-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-services-core-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-mawo-core-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-services-api-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-globalpolicygenerator-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-router-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-3.4.1.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r 4d7825309348956336b8f06a08322b78422849b1; compiled by 'mthakur' on 2024-10-09T14:57Z
STARTUP_MSG:   java = 1.8.0_212
************************************************************/
2025-04-13 05:54:31 INFO  NameNode:91 - registered UNIX signal handlers for [TERM, HUP, INT]
2025-04-13 05:54:32 INFO  NameNode:1846 - createNameNode [-format, -force, ]
2025-04-13 05:54:37 INFO  NameNode:1393 - Formatting using clusterid: CID-ff5816d9-e213-4463-b4f6-bac2bec62c1b
2025-04-13 05:54:37 INFO  FSEditLog:238 - Edit logging is async:true
2025-04-13 05:54:37 INFO  FSNamesystem:869 - KeyProvider: null
2025-04-13 05:54:37 INFO  FSNamesystem:142 - fsLock is fair: true
2025-04-13 05:54:37 INFO  FSNamesystem:160 - Detailed lock hold time metrics enabled: false
2025-04-13 05:54:37 INFO  FSNamesystem:904 - fsOwner                = hadoop (auth:SIMPLE)
2025-04-13 05:54:37 INFO  FSNamesystem:905 - supergroup             = supergroup
2025-04-13 05:54:37 INFO  FSNamesystem:906 - isPermissionEnabled    = true
2025-04-13 05:54:37 INFO  FSNamesystem:907 - isStoragePolicyEnabled = true
2025-04-13 05:54:37 INFO  FSNamesystem:918 - HA Enabled: false
2025-04-13 05:54:38 INFO  Util:428 - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2025-04-13 05:54:39 INFO  DatanodeManager:412 - Slow peers collection thread shutdown
2025-04-13 05:54:39 INFO  DatanodeManager:2182 - dfs.block.invalidate.limit : configured=1000, counted=60, effected=1000
2025-04-13 05:54:39 INFO  DatanodeManager:320 - dfs.namenode.datanode.registration.ip-hostname-check=true
2025-04-13 05:54:39 INFO  BlockManager:77 - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2025-04-13 05:54:39 INFO  BlockManager:83 - The block deletion will start around 2025 Apr 13 05:54:39
2025-04-13 05:54:39 INFO  GSet:408 - Computing capacity for map BlocksMap
2025-04-13 05:54:39 INFO  GSet:409 - VM type       = 64-bit
2025-04-13 05:54:39 INFO  GSet:410 - 2.0% max memory 1.0 GB = 21.3 MB
2025-04-13 05:54:39 INFO  GSet:415 - capacity      = 2^21 = 2097152 entries
2025-04-13 05:54:39 INFO  BlockManager:5778 - Storage policy satisfier is disabled
2025-04-13 05:54:39 INFO  BlockManager:696 - dfs.block.access.token.enable = false
2025-04-13 05:54:39 INFO  BlockManagerSafeMode:656 - Using 1000 as SafeModeMonitor Interval
2025-04-13 05:54:39 INFO  BlockManagerSafeMode:161 - dfs.namenode.safemode.threshold-pct = 0.999
2025-04-13 05:54:39 INFO  BlockManagerSafeMode:162 - dfs.namenode.safemode.min.datanodes = 0
2025-04-13 05:54:39 INFO  BlockManagerSafeMode:164 - dfs.namenode.safemode.extension = 30000
2025-04-13 05:54:39 INFO  BlockManager:682 - defaultReplication         = 3
2025-04-13 05:54:39 INFO  BlockManager:683 - maxReplication             = 512
2025-04-13 05:54:39 INFO  BlockManager:684 - minReplication             = 1
2025-04-13 05:54:39 INFO  BlockManager:685 - maxReplicationStreams      = 2
2025-04-13 05:54:39 INFO  BlockManager:686 - redundancyRecheckInterval  = 3000ms
2025-04-13 05:54:39 INFO  BlockManager:687 - encryptDataTransfer        = false
2025-04-13 05:54:39 INFO  BlockManager:688 - maxNumBlocksToLog          = 1000
2025-04-13 05:54:39 INFO  FSDirectory:51 - GLOBAL serial map: bits=29 maxEntries=536870911
2025-04-13 05:54:39 INFO  FSDirectory:51 - USER serial map: bits=24 maxEntries=16777215
2025-04-13 05:54:39 INFO  FSDirectory:51 - GROUP serial map: bits=24 maxEntries=16777215
2025-04-13 05:54:39 INFO  FSDirectory:51 - XATTR serial map: bits=24 maxEntries=16777215
2025-04-13 05:54:39 INFO  GSet:408 - Computing capacity for map INodeMap
2025-04-13 05:54:39 INFO  GSet:409 - VM type       = 64-bit
2025-04-13 05:54:39 INFO  GSet:410 - 1.0% max memory 1.0 GB = 10.7 MB
2025-04-13 05:54:39 INFO  GSet:415 - capacity      = 2^20 = 1048576 entries
2025-04-13 05:54:39 INFO  FSDirectory:339 - ACLs enabled? true
2025-04-13 05:54:40 INFO  FSDirectory:343 - POSIX ACL inheritance enabled? true
2025-04-13 05:54:40 INFO  FSDirectory:347 - XAttrs enabled? true
2025-04-13 05:54:40 INFO  NameNode:414 - Caching file names occurring more than 10 times
2025-04-13 05:54:40 INFO  SnapshotManager:163 - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotFSLimit: 65536, maxSnapshotLimit: 65536
2025-04-13 05:54:40 INFO  SnapshotManager:176 - dfs.namenode.snapshot.deletion.ordered = false
2025-04-13 05:54:40 INFO  SnapshotManager:43 - SkipList is disabled
2025-04-13 05:54:40 INFO  GSet:408 - Computing capacity for map cachedBlocks
2025-04-13 05:54:40 INFO  GSet:409 - VM type       = 64-bit
2025-04-13 05:54:40 INFO  GSet:410 - 0.25% max memory 1.0 GB = 2.7 MB
2025-04-13 05:54:40 INFO  GSet:415 - capacity      = 2^18 = 262144 entries
2025-04-13 05:54:40 INFO  TopMetrics:76 - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2025-04-13 05:54:40 INFO  TopMetrics:78 - NNTop conf: dfs.namenode.top.num.users = 10
2025-04-13 05:54:40 INFO  TopMetrics:80 - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2025-04-13 05:54:40 INFO  FSNamesystem:1158 - Retry cache on namenode is enabled
2025-04-13 05:54:40 INFO  FSNamesystem:1166 - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2025-04-13 05:54:40 INFO  GSet:408 - Computing capacity for map NameNodeRetryCache
2025-04-13 05:54:40 INFO  GSet:409 - VM type       = 64-bit
2025-04-13 05:54:40 INFO  GSet:410 - 0.029999999329447746% max memory 1.0 GB = 327.8 KB
2025-04-13 05:54:40 INFO  GSet:415 - capacity      = 2^15 = 32768 entries
2025-04-13 05:54:40 INFO  FSImage:186 - Allocated new BlockPoolId: BP-262012374-172.19.0.2-1744523680573
2025-04-13 05:54:40 INFO  Storage:595 - Storage directory /opt/hdfs/namenode has been successfully formatted.
2025-04-13 05:54:41 INFO  FSImageFormatProtobuf:732 - Saving image file /opt/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression
2025-04-13 05:54:42 INFO  FSImageFormatProtobuf:736 - Image file /opt/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 398 bytes saved in 0 seconds .
2025-04-13 05:54:42 INFO  NNStorageRetentionManager:205 - Going to retain 1 images with txid >= 0
2025-04-13 05:54:42 INFO  DatanodeManager:412 - Slow peers collection thread shutdown
2025-04-13 05:54:42 INFO  FSNamesystem:1509 - Stopping services started for active state
2025-04-13 05:54:42 INFO  FSNamesystem:1613 - Stopping services started for standby state
2025-04-13 05:54:42 INFO  FSImage:1057 - FSImageSaver clean checkpoint: txid=0 when meet shutdown.
2025-04-13 05:54:42 INFO  NameNode:822 - SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at hadoop-namenode/172.19.0.2
************************************************************/
2025-04-13 05:54:47 INFO  NameNode:809 - STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = hadoop-namenode/172.19.0.2
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.4.1
STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/netty-common-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/kerby-asn1-2.0.3.jar:/opt/hadoop/share/hadoop/common/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.100.Final-linux-x86_64.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.100.Final-linux-aarch_64.jar:/opt/hadoop/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-security-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-mqtt-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-haproxy-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-http2-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.9.0.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-dns-classes-macos-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-3.4.1.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.9.2.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-http-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/checker-qual-2.5.2.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-classes-kqueue-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-redis-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-xml-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.55.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-dns-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-5.2.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.9.0.jar:/opt/hadoop/share/hadoop/common/lib/kerb-core-2.0.3.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-aarch_64.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration2-2.10.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.26.1.jar:/opt/hadoop/share/hadoop/common/lib/bcprov-jdk18on-1.78.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/common/lib/jetty-http-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/common/lib/kerby-pkix-2.0.3.jar:/opt/hadoop/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/hadoop/common/lib/dnsjava-3.6.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-unix-common-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.19.4.jar:/opt/hadoop/share/hadoop/common/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang3-3.12.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-jute-3.8.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.5.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-servlet-1.19.4.jar:/opt/hadoop/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop/share/hadoop/common/lib/jetty-xml-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/common/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-buffer-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-handler-ssl-ocsp-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/jetty-server-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.19.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.6.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-5.2.0.jar:/opt/hadoop/share/hadoop/common/lib/netty-handler-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.13.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.100.Final-osx-x86_64.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-ajax-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-udt-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-dns-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-sctp-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/kerby-util-2.0.3.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.2.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.8.4.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-smtp-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-5.2.0.jar:/opt/hadoop/share/hadoop/common/lib/failureaccess-1.0.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-classes-epoll-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.22.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.15.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.1.10.4.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-rxtx-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-socks-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.16.1.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/re2j-1.1.jar:/opt/hadoop/share/hadoop/common/lib/kerb-crypto-2.0.3.jar:/opt/hadoop/share/hadoop/common/lib/jetty-servlet-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-2.12.7.jar:/opt/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar:/opt/hadoop/share/hadoop/common/lib/reload4j-1.2.22.jar:/opt/hadoop/share/hadoop/common/lib/jackson-databind-2.12.7.1.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-9.37.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-text-1.10.0.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-3.4.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-memcache-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.5.4.jar:/opt/hadoop/share/hadoop/common/lib/kerby-config-2.0.3.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.13.jar:/opt/hadoop/share/hadoop/common/lib/netty-handler-proxy-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-stomp-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/jul-to-slf4j-1.7.36.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/kerb-util-2.0.3.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-x86_64.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-shaded-protobuf_3_25-1.3.0.jar:/opt/hadoop/share/hadoop/common/lib/audience-annotations-0.12.0.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.2.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.100.Final-osx-aarch_64.jar:/opt/hadoop/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-io-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/common/lib/netty-all-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-shaded-guava-1.3.0.jar:/opt/hadoop/share/hadoop/common/lib/jackson-annotations-2.12.7.jar:/opt/hadoop/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/hadoop/common/lib/jetty-webapp-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-3.4.1.jar:/opt/hadoop/share/hadoop/common/hadoop-common-3.4.1.jar:/opt/hadoop/share/hadoop/common/hadoop-common-3.4.1-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-registry-3.4.1.jar:/opt/hadoop/share/hadoop/common/hadoop-kms-3.4.1.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/netty-common-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-asn1-2.0.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.100.Final-linux-x86_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.100.Final-linux-aarch_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-security-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-mqtt-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-haproxy-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-http2-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/gson-2.9.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-classes-macos-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-auth-3.4.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/avro-1.9.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-http-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-classes-kqueue-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-redis-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-xml-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-dns-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-client-5.2.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-net-3.9.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-core-2.0.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-aarch_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-configuration2-2.10.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-compress-1.26.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-http-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-pkix-2.0.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/hadoop/hdfs/lib/dnsjava-3.6.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-unix-common-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.19.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang3-3.12.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/zookeeper-jute-3.8.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-servlet-1.19.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-xml-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-buffer-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-handler-ssl-ocsp-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-server-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.19.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-math3-3.6.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-recipes-5.2.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/HikariCP-4.0.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-handler-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.100.Final-osx-x86_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-udt-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-sctp-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-util-2.0.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/zookeeper-3.8.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-smtp-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-framework-5.2.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-classes-epoll-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-json-1.22.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.15.jar:/opt/hadoop/share/hadoop/hdfs/lib/snappy-java-1.1.10.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-rxtx-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-socks-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.16.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/hdfs/lib/re2j-1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-crypto-2.0.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-servlet-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-2.12.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/opt/hadoop/share/hadoop/hdfs/lib/reload4j-1.2.22.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-databind-2.12.7.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.37.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-text-1.10.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-annotations-3.4.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-memcache-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jettison-1.5.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-config-2.0.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-handler-proxy-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-stomp-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-util-2.0.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-x86_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_25-1.3.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/audience-annotations-0.12.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.100.Final-osx-aarch_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-io-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-shaded-guava-1.3.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-annotations-2.12.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-webapp-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.4.1.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.4.1.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.4.1-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.4.1-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.4.1-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.4.1-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-3.4.1.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.4.1.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.4.1.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.4.1-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.4.1.jar:/opt/hadoop/share/hadoop/yarn:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.12.7.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-jndi-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/yarn/lib/javax-websocket-server-impl-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-api-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.websocket-client-api-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/cache-api-1.1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-server-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-base-2.12.7.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-annotations-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/yarn/lib/javax-websocket-client-impl-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/yarn/lib/ehcache-3.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/FastInfoset-1.2.15.jar:/opt/hadoop/share/hadoop/yarn/lib/snakeyaml-2.0.jar:/opt/hadoop/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-servlet-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-commons-9.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-4.2.3.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.19.4.jar:/opt/hadoop/share/hadoop/yarn/lib/jna-5.2.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-tree-9.6.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-plus-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/yarn/lib/istack-commons-runtime-3.0.7.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-ex-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/txw2-2.3.1.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-client-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-client-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/yarn/lib/bcutil-jdk18on-1.78.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jline-3.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/codemodel-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.websocket-api-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-4.2.3.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.12.7.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.19.4.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-runtime-2.3.1.jar:/opt/hadoop/share/hadoop/yarn/lib/bcpkix-jdk18on-1.78.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsonschema2pojo-core-1.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/hadoop/yarn/lib/objenesis-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-common-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-services-core-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-mawo-core-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-services-api-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-globalpolicygenerator-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-router-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-3.4.1.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r 4d7825309348956336b8f06a08322b78422849b1; compiled by 'mthakur' on 2024-10-09T14:57Z
STARTUP_MSG:   java = 1.8.0_212
************************************************************/
2025-04-13 05:54:47 INFO  NameNode:91 - registered UNIX signal handlers for [TERM, HUP, INT]
2025-04-13 05:54:48 INFO  NameNode:1846 - createNameNode []
2025-04-13 05:54:50 INFO  MetricsConfig:122 - Loaded properties from hadoop-metrics2.properties
2025-04-13 05:54:51 INFO  MetricsSystemImpl:378 - Scheduled Metric snapshot period at 10 second(s).
2025-04-13 05:54:51 INFO  MetricsSystemImpl:191 - NameNode metrics system started
2025-04-13 05:54:51 INFO  NameNodeUtils:79 - fs.defaultFS is hdfs://hadoop-namenode:9000
2025-04-13 05:54:51 INFO  NameNode:1149 - Clients should use hadoop-namenode:9000 to access this namenode/service.
2025-04-13 05:54:53 INFO  JvmPauseMonitor:185 - Starting JVM pause monitor
2025-04-13 05:54:53 INFO  DFSUtil:1769 - Filter initializers set : org.apache.hadoop.http.lib.StaticUserWebFilter,org.apache.hadoop.hdfs.web.AuthFilterInitializer
2025-04-13 05:54:53 INFO  DFSUtil:1791 - Starting Web-server for hdfs at: http://0.0.0.0:9870
2025-04-13 05:54:53 INFO  log:170 - Logging initialized @10543ms to org.eclipse.jetty.util.log.Slf4jLog
2025-04-13 05:54:54 WARN  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
2025-04-13 05:54:55 INFO  HttpServer2:1205 - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2025-04-13 05:54:55 INFO  HttpServer2:1178 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2025-04-13 05:54:55 INFO  HttpServer2:1188 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2025-04-13 05:54:55 INFO  HttpServer2:1188 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2025-04-13 05:54:55 INFO  HttpServer2:1178 - Added filter AuthFilter (class=org.apache.hadoop.hdfs.web.AuthFilter) to context hdfs
2025-04-13 05:54:55 INFO  HttpServer2:1188 - Added filter AuthFilter (class=org.apache.hadoop.hdfs.web.AuthFilter) to context logs
2025-04-13 05:54:55 INFO  HttpServer2:1188 - Added filter AuthFilter (class=org.apache.hadoop.hdfs.web.AuthFilter) to context static
2025-04-13 05:54:55 INFO  HttpServer2:813 - ASYNC_PROFILER_HOME environment variable and async.profiler.home system property not specified. Disabling /prof endpoint.
2025-04-13 05:54:55 INFO  HttpServer2:1032 - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2025-04-13 05:54:55 INFO  HttpServer2:1452 - Jetty bound to port 9870
2025-04-13 05:54:55 INFO  Server:375 - jetty-9.4.53.v20231009; built: 2023-10-09T12:29:09.265Z; git: 27bde00a0b95a1d5bbee0eae7984f891d2d0f8c9; jvm 1.8.0_212-b04
2025-04-13 05:54:56 INFO  session:334 - DefaultSessionIdManager workerName=node0
2025-04-13 05:54:56 INFO  session:339 - No SessionScavenger set, using defaults
2025-04-13 05:54:56 INFO  session:132 - node0 Scavenging every 660000ms
2025-04-13 05:54:56 WARN  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
2025-04-13 05:54:56 INFO  ContextHandler:921 - Started o.e.j.s.ServletContextHandler@9816741{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
2025-04-13 05:54:56 INFO  ContextHandler:921 - Started o.e.j.s.ServletContextHandler@7dc3712{static,/static,file:///opt/hadoop/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2025-04-13 05:54:57 INFO  ContextHandler:921 - Started o.e.j.w.WebAppContext@20c0a64d{hdfs,/,file:///opt/hadoop/share/hadoop/hdfs/webapps/hdfs/,AVAILABLE}{file:/opt/hadoop/share/hadoop/hdfs/webapps/hdfs}
2025-04-13 05:54:57 INFO  AbstractConnector:333 - Started ServerConnector@6ca18a14{HTTP/1.1, (http/1.1)}{0.0.0.0:9870}
2025-04-13 05:54:57 INFO  Server:415 - Started @14044ms
2025-04-13 05:54:58 WARN  FSNamesystem:800 - Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2025-04-13 05:54:58 WARN  FSNamesystem:805 - Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2025-04-13 05:54:59 INFO  FSEditLog:238 - Edit logging is async:true
2025-04-13 05:54:59 INFO  FSNamesystem:869 - KeyProvider: null
2025-04-13 05:54:59 INFO  FSNamesystem:142 - fsLock is fair: true
2025-04-13 05:54:59 INFO  FSNamesystem:160 - Detailed lock hold time metrics enabled: false
2025-04-13 05:54:59 INFO  FSNamesystem:904 - fsOwner                = hadoop (auth:SIMPLE)
2025-04-13 05:54:59 INFO  FSNamesystem:905 - supergroup             = supergroup
2025-04-13 05:54:59 INFO  FSNamesystem:906 - isPermissionEnabled    = true
2025-04-13 05:54:59 INFO  FSNamesystem:907 - isStoragePolicyEnabled = true
2025-04-13 05:54:59 INFO  FSNamesystem:918 - HA Enabled: false
2025-04-13 05:54:59 INFO  Util:428 - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2025-04-13 05:55:01 INFO  DatanodeManager:412 - Slow peers collection thread shutdown
2025-04-13 05:55:01 INFO  DatanodeManager:2182 - dfs.block.invalidate.limit : configured=1000, counted=60, effected=1000
2025-04-13 05:55:01 INFO  DatanodeManager:320 - dfs.namenode.datanode.registration.ip-hostname-check=true
2025-04-13 05:55:01 INFO  BlockManager:77 - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2025-04-13 05:55:01 INFO  BlockManager:83 - The block deletion will start around 2025 Apr 13 05:55:01
2025-04-13 05:55:01 INFO  GSet:408 - Computing capacity for map BlocksMap
2025-04-13 05:55:01 INFO  GSet:409 - VM type       = 64-bit
2025-04-13 05:55:01 INFO  GSet:410 - 2.0% max memory 1.0 GB = 21.3 MB
2025-04-13 05:55:01 INFO  GSet:415 - capacity      = 2^21 = 2097152 entries
2025-04-13 05:55:01 INFO  BlockManager:5778 - Storage policy satisfier is disabled
2025-04-13 05:55:01 INFO  BlockManager:696 - dfs.block.access.token.enable = false
2025-04-13 05:55:01 INFO  BlockManagerSafeMode:656 - Using 1000 as SafeModeMonitor Interval
2025-04-13 05:55:01 INFO  BlockManagerSafeMode:161 - dfs.namenode.safemode.threshold-pct = 0.999
2025-04-13 05:55:01 INFO  BlockManagerSafeMode:162 - dfs.namenode.safemode.min.datanodes = 0
2025-04-13 05:55:01 INFO  BlockManagerSafeMode:164 - dfs.namenode.safemode.extension = 30000
2025-04-13 05:55:01 INFO  BlockManager:682 - defaultReplication         = 3
2025-04-13 05:55:01 INFO  BlockManager:683 - maxReplication             = 512
2025-04-13 05:55:01 INFO  BlockManager:684 - minReplication             = 1
2025-04-13 05:55:01 INFO  BlockManager:685 - maxReplicationStreams      = 2
2025-04-13 05:55:01 INFO  BlockManager:686 - redundancyRecheckInterval  = 3000ms
2025-04-13 05:55:01 INFO  BlockManager:687 - encryptDataTransfer        = false
2025-04-13 05:55:01 INFO  BlockManager:688 - maxNumBlocksToLog          = 1000
2025-04-13 05:55:01 INFO  FSDirectory:51 - GLOBAL serial map: bits=29 maxEntries=536870911
2025-04-13 05:55:01 INFO  FSDirectory:51 - USER serial map: bits=24 maxEntries=16777215
2025-04-13 05:55:01 INFO  FSDirectory:51 - GROUP serial map: bits=24 maxEntries=16777215
2025-04-13 05:55:01 INFO  FSDirectory:51 - XATTR serial map: bits=24 maxEntries=16777215
2025-04-13 05:55:01 INFO  GSet:408 - Computing capacity for map INodeMap
2025-04-13 05:55:02 INFO  GSet:409 - VM type       = 64-bit
2025-04-13 05:55:02 INFO  GSet:410 - 1.0% max memory 1.0 GB = 10.7 MB
2025-04-13 05:55:02 INFO  GSet:415 - capacity      = 2^20 = 1048576 entries
2025-04-13 05:55:02 INFO  FSDirectory:339 - ACLs enabled? true
2025-04-13 05:55:02 INFO  FSDirectory:343 - POSIX ACL inheritance enabled? true
2025-04-13 05:55:02 INFO  FSDirectory:347 - XAttrs enabled? true
2025-04-13 05:55:02 INFO  NameNode:414 - Caching file names occurring more than 10 times
2025-04-13 05:55:02 INFO  SnapshotManager:163 - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotFSLimit: 65536, maxSnapshotLimit: 65536
2025-04-13 05:55:02 INFO  SnapshotManager:176 - dfs.namenode.snapshot.deletion.ordered = false
2025-04-13 05:55:02 INFO  SnapshotManager:43 - SkipList is disabled
2025-04-13 05:55:02 INFO  GSet:408 - Computing capacity for map cachedBlocks
2025-04-13 05:55:02 INFO  GSet:409 - VM type       = 64-bit
2025-04-13 05:55:02 INFO  GSet:410 - 0.25% max memory 1.0 GB = 2.7 MB
2025-04-13 05:55:02 INFO  GSet:415 - capacity      = 2^18 = 262144 entries
2025-04-13 05:55:02 INFO  TopMetrics:76 - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2025-04-13 05:55:02 INFO  TopMetrics:78 - NNTop conf: dfs.namenode.top.num.users = 10
2025-04-13 05:55:02 INFO  TopMetrics:80 - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2025-04-13 05:55:02 INFO  FSNamesystem:1158 - Retry cache on namenode is enabled
2025-04-13 05:55:02 INFO  FSNamesystem:1166 - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2025-04-13 05:55:02 INFO  GSet:408 - Computing capacity for map NameNodeRetryCache
2025-04-13 05:55:02 INFO  GSet:409 - VM type       = 64-bit
2025-04-13 05:55:02 INFO  GSet:410 - 0.029999999329447746% max memory 1.0 GB = 327.8 KB
2025-04-13 05:55:02 INFO  GSet:415 - capacity      = 2^15 = 32768 entries
2025-04-13 05:55:02 INFO  Storage:948 - Lock on /opt/hdfs/namenode/in_use.lock acquired by nodename 72@hadoop-namenode
2025-04-13 05:55:02 INFO  FileJournalManager:428 - Recovering unfinalized segments in /opt/hdfs/namenode/current
2025-04-13 05:55:02 INFO  FSImage:734 - No edit log streams selected.
2025-04-13 05:55:02 INFO  FSImage:800 - Planning to load image: FSImageFile(file=/opt/hdfs/namenode/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2025-04-13 05:55:03 INFO  FSImageFormatPBINode:411 - Loading 1 INodes.
2025-04-13 05:55:03 INFO  FSImageFormatPBINode:369 - Successfully loaded 1 inodes
2025-04-13 05:55:03 INFO  FSImageFormatPBINode:342 - Completed update blocks map and name cache, total waiting duration 0ms.
2025-04-13 05:55:03 INFO  FSImageFormatProtobuf:255 - Loaded FSImage in 0 seconds.
2025-04-13 05:55:03 INFO  FSImage:980 - Loaded image for txid 0 from /opt/hdfs/namenode/current/fsimage_0000000000000000000
2025-04-13 05:55:03 INFO  FSNamesystem:1280 - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2025-04-13 05:55:03 INFO  FSEditLog:1417 - Starting log segment at 1
2025-04-13 05:55:04 INFO  NameCache:143 - initialized with 0 entries 0 lookups
2025-04-13 05:55:04 INFO  FSNamesystem:841 - Finished loading FSImage in 1640 msecs
2025-04-13 05:55:05 INFO  NameNode:452 - RPC server is binding to hadoop-namenode:9000
2025-04-13 05:55:05 INFO  NameNode:457 - Enable NameNode state context:false
2025-04-13 05:55:05 INFO  CallQueueManager:93 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false, ipcFailOver: false.
2025-04-13 05:55:05 INFO  Server:1438 - Listener at hadoop-namenode:9000
2025-04-13 05:55:06 INFO  Server:1474 - Starting Socket Reader #1 for port 9000
2025-04-13 05:55:07 INFO  FSNamesystem:5656 - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2025-04-13 05:55:07 INFO  LeaseManager:166 - Number of blocks under construction: 0
2025-04-13 05:55:07 INFO  DatanodeAdminDefaultMonitor:126 - Initialized the Default Decommission and Maintenance monitor
2025-04-13 05:55:07 INFO  BlockManager:5296 - Start MarkedDeleteBlockScrubber thread
2025-04-13 05:55:07 INFO  BlockManager:5505 - initializing replication queues
2025-04-13 05:55:07 INFO  StateChange:409 - STATE* Leaving safe mode after 0 secs
2025-04-13 05:55:07 INFO  StateChange:415 - STATE* Network topology has 0 racks and 0 datanodes
2025-04-13 05:55:07 INFO  StateChange:417 - STATE* UnderReplicatedBlocks has 0 blocks
2025-04-13 05:55:07 INFO  BlockManager:4030 - Total number of blocks            = 0
2025-04-13 05:55:07 INFO  BlockManager:4031 - Number of invalid blocks          = 0
2025-04-13 05:55:07 INFO  BlockManager:4032 - Number of under-replicated blocks = 0
2025-04-13 05:55:07 INFO  BlockManager:4033 - Number of  over-replicated blocks = 0
2025-04-13 05:55:07 INFO  BlockManager:4035 - Number of blocks being written    = 0
2025-04-13 05:55:07 INFO  StateChange:4038 - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 134 msec
2025-04-13 05:55:07 INFO  BlockManager:4047 - Reconstruction queues initialisation progress: 0.0, total number of blocks processed: 0/0
2025-04-13 05:55:08 INFO  Server:1713 - IPC Server Responder: starting
2025-04-13 05:55:08 INFO  Server:1553 - IPC Server listener on 9000: starting
2025-04-13 05:55:08 INFO  NameNode:1031 - NameNode RPC up at: hadoop-namenode/172.19.0.2:9000.
2025-04-13 05:55:08 INFO  FSNamesystem:1392 - Starting services required for active state
2025-04-13 05:55:08 INFO  FSDirectory:857 - Initializing quota with 12 thread(s)
2025-04-13 05:55:08 INFO  FSDirectory:866 - Quota initialization completed in 94 milliseconds
name space=1
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0, NVDIMM=0
2025-04-13 05:55:08 INFO  CacheReplicationMonitor:165 - Starting CacheReplicationMonitor with interval 30000 milliseconds
2025-04-13 05:55:11 INFO  StateChange:1188 - BLOCK* registerDatanode: from DatanodeRegistration(172.19.0.3:9971, datanodeUuid=fa0d5a95-193a-4927-84cf-fd8f2267ab44, infoPort=9865, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-ff5816d9-e213-4463-b4f6-bac2bec62c1b;nsid=117484653;c=1744523680573) storage fa0d5a95-193a-4927-84cf-fd8f2267ab44
2025-04-13 05:55:11 INFO  NetworkTopology:156 - Adding a new node: /default-rack/172.19.0.3:9971
2025-04-13 05:55:11 INFO  BlockReportLeaseManager:200 - Registered DN fa0d5a95-193a-4927-84cf-fd8f2267ab44 (172.19.0.3:9971).
2025-04-13 05:55:11 INFO  StateChange:1188 - BLOCK* registerDatanode: from DatanodeRegistration(172.19.0.4:9970, datanodeUuid=39a6a668-39fb-4c52-9d8a-8b7f0a867bba, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-ff5816d9-e213-4463-b4f6-bac2bec62c1b;nsid=117484653;c=1744523680573) storage 39a6a668-39fb-4c52-9d8a-8b7f0a867bba
2025-04-13 05:55:11 INFO  NetworkTopology:156 - Adding a new node: /default-rack/172.19.0.4:9970
2025-04-13 05:55:11 INFO  BlockReportLeaseManager:200 - Registered DN 39a6a668-39fb-4c52-9d8a-8b7f0a867bba (172.19.0.4:9970).
2025-04-13 05:55:11 INFO  StateChange:1188 - BLOCK* registerDatanode: from DatanodeRegistration(172.19.0.5:9972, datanodeUuid=11da33f8-8e91-48e2-9cf3-9a0d13996193, infoPort=9866, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-ff5816d9-e213-4463-b4f6-bac2bec62c1b;nsid=117484653;c=1744523680573) storage 11da33f8-8e91-48e2-9cf3-9a0d13996193
2025-04-13 05:55:11 INFO  NetworkTopology:156 - Adding a new node: /default-rack/172.19.0.5:9972
2025-04-13 05:55:11 INFO  BlockReportLeaseManager:200 - Registered DN 11da33f8-8e91-48e2-9cf3-9a0d13996193 (172.19.0.5:9972).
2025-04-13 05:55:11 INFO  DatanodeDescriptor:1065 - Adding new storage ID DS-74b66c21-6b61-4107-a7bb-452ff5841ec0 for DN 172.19.0.5:9972
2025-04-13 05:55:11 INFO  DatanodeDescriptor:1065 - Adding new storage ID DS-3ee6931e-8c5a-460e-b62d-eeb5a7f672d8 for DN 172.19.0.3:9971
2025-04-13 05:55:11 INFO  DatanodeDescriptor:1065 - Adding new storage ID DS-2f905a4a-1ad5-4ad3-a48c-b01b7fc6dab4 for DN 172.19.0.4:9970
2025-04-13 05:55:11 INFO  BlockStateChange:2940 - BLOCK* processReport 0xcafe254fc11b4d60 with lease ID 0xb94ee93497f22eb9: Processing first storage report for DS-74b66c21-6b61-4107-a7bb-452ff5841ec0 from datanode DatanodeRegistration(172.19.0.5:9972, datanodeUuid=11da33f8-8e91-48e2-9cf3-9a0d13996193, infoPort=9866, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-ff5816d9-e213-4463-b4f6-bac2bec62c1b;nsid=117484653;c=1744523680573)
2025-04-13 05:55:11 INFO  BlockStateChange:2972 - BLOCK* processReport 0xcafe254fc11b4d60 with lease ID 0xb94ee93497f22eb9: from storage DS-74b66c21-6b61-4107-a7bb-452ff5841ec0 node DatanodeRegistration(172.19.0.5:9972, datanodeUuid=11da33f8-8e91-48e2-9cf3-9a0d13996193, infoPort=9866, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-ff5816d9-e213-4463-b4f6-bac2bec62c1b;nsid=117484653;c=1744523680573), blocks: 0, hasStaleStorage: false, processing time: 9 msecs, invalidatedBlocks: 0
2025-04-13 05:55:11 INFO  BlockStateChange:2940 - BLOCK* processReport 0x22608eb50527deeb with lease ID 0xb94ee93497f22eba: Processing first storage report for DS-3ee6931e-8c5a-460e-b62d-eeb5a7f672d8 from datanode DatanodeRegistration(172.19.0.3:9971, datanodeUuid=fa0d5a95-193a-4927-84cf-fd8f2267ab44, infoPort=9865, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-ff5816d9-e213-4463-b4f6-bac2bec62c1b;nsid=117484653;c=1744523680573)
2025-04-13 05:55:11 INFO  BlockStateChange:2972 - BLOCK* processReport 0x22608eb50527deeb with lease ID 0xb94ee93497f22eba: from storage DS-3ee6931e-8c5a-460e-b62d-eeb5a7f672d8 node DatanodeRegistration(172.19.0.3:9971, datanodeUuid=fa0d5a95-193a-4927-84cf-fd8f2267ab44, infoPort=9865, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-ff5816d9-e213-4463-b4f6-bac2bec62c1b;nsid=117484653;c=1744523680573), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2025-04-13 05:55:11 INFO  BlockStateChange:2940 - BLOCK* processReport 0x540ccd678d295e2d with lease ID 0xb94ee93497f22ebb: Processing first storage report for DS-2f905a4a-1ad5-4ad3-a48c-b01b7fc6dab4 from datanode DatanodeRegistration(172.19.0.4:9970, datanodeUuid=39a6a668-39fb-4c52-9d8a-8b7f0a867bba, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-ff5816d9-e213-4463-b4f6-bac2bec62c1b;nsid=117484653;c=1744523680573)
2025-04-13 05:55:11 INFO  BlockStateChange:2972 - BLOCK* processReport 0x540ccd678d295e2d with lease ID 0xb94ee93497f22ebb: from storage DS-2f905a4a-1ad5-4ad3-a48c-b01b7fc6dab4 node DatanodeRegistration(172.19.0.4:9970, datanodeUuid=39a6a668-39fb-4c52-9d8a-8b7f0a867bba, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-ff5816d9-e213-4463-b4f6-bac2bec62c1b;nsid=117484653;c=1744523680573), blocks: 0, hasStaleStorage: false, processing time: 3 msecs, invalidatedBlocks: 0
2025-04-13 06:11:11 INFO  Interns:50 - Metrics intern cache overflow at 2011 for MetricsSystem={MetricsSystem=MetricsInfoImpl{name=MetricsSystem, description=MetricsSystem}, MetricsSystem record=MetricsInfoImpl{name=MetricsSystem, description=MetricsSystem record}}
2025-04-13 06:11:11 INFO  FSEditLog:801 - Number of transactions: 2 Total time for transactions(ms): 33 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 34 
2025-04-13 06:11:11 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_a7e8ceae-34a7-4268-9cc6-fb7cb69e0856	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:11:11 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:11 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:11 INFO  StateChange:801 - BLOCK* allocate blk_1073741825_1001, replicas=172.19.0.3:9971, 172.19.0.5:9972, 172.19.0.4:9970 for /data/message_a7e8ceae-34a7-4268-9cc6-fb7cb69e0856
2025-04-13 06:11:12 INFO  FSNamesystem:3286 - BLOCK* blk_1073741825_1001 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /data/message_a7e8ceae-34a7-4268-9cc6-fb7cb69e0856
2025-04-13 06:11:12 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.5:9972 is added to blk_1073741825_1001 (size=72)
2025-04-13 06:11:12 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741825_1001 (size=72)
2025-04-13 06:11:12 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741825_1001 (size=72)
2025-04-13 06:11:12 INFO  StateChange:3239 - DIR* completeFile: /data/message_a7e8ceae-34a7-4268-9cc6-fb7cb69e0856 is closed by DFSClient_NONMAPREDUCE_-1706211797_1
2025-04-13 06:11:12 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_77e1cf73-6795-4781-9592-c5d15c41c976	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:11:12 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:12 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:12 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:12 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2025-04-13 06:11:12 WARN  BlockStoragePolicy:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2025-04-13 06:11:12 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2025-04-13 06:11:12 INFO  StateChange:801 - BLOCK* allocate blk_1073741826_1002, replicas=172.19.0.4:9970, 172.19.0.5:9972 for /data/message_77e1cf73-6795-4781-9592-c5d15c41c976
2025-04-13 06:11:12 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.5:9972 is added to blk_1073741826_1002 (size=69)
2025-04-13 06:11:12 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741826_1002 (size=69)
2025-04-13 06:11:12 INFO  StateChange:3239 - DIR* completeFile: /data/message_77e1cf73-6795-4781-9592-c5d15c41c976 is closed by DFSClient_NONMAPREDUCE_-1706211797_1
2025-04-13 06:11:12 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_2c118040-1dd7-4e4e-bb73-091d98f7e261	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:11:12 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:12 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:12 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:12 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2025-04-13 06:11:12 WARN  BlockStoragePolicy:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2025-04-13 06:11:12 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2025-04-13 06:11:12 INFO  StateChange:801 - BLOCK* allocate blk_1073741827_1003, replicas=172.19.0.4:9970, 172.19.0.5:9972 for /data/message_2c118040-1dd7-4e4e-bb73-091d98f7e261
2025-04-13 06:11:12 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.5:9972 is added to blk_1073741827_1003 (size=72)
2025-04-13 06:11:12 INFO  StateChange:3239 - DIR* completeFile: /data/message_2c118040-1dd7-4e4e-bb73-091d98f7e261 is closed by DFSClient_NONMAPREDUCE_-1706211797_1
2025-04-13 06:11:12 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_bd148af6-6479-445f-91b4-86d1639b2a6f	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:11:12 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741827_1003 (size=72)
2025-04-13 06:11:12 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:12 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:12 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:12 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2025-04-13 06:11:12 WARN  BlockStoragePolicy:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2025-04-13 06:11:12 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2025-04-13 06:11:12 INFO  StateChange:801 - BLOCK* allocate blk_1073741828_1004, replicas=172.19.0.5:9972, 172.19.0.4:9970 for /data/message_bd148af6-6479-445f-91b4-86d1639b2a6f
2025-04-13 06:11:12 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741828_1004 (size=72)
2025-04-13 06:11:12 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.5:9972 is added to blk_1073741828_1004 (size=72)
2025-04-13 06:11:12 INFO  StateChange:3239 - DIR* completeFile: /data/message_bd148af6-6479-445f-91b4-86d1639b2a6f is closed by DFSClient_NONMAPREDUCE_-1706211797_1
2025-04-13 06:11:12 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_15d56695-89df-48ee-88a1-984f5f91e175	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:11:12 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:12 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:12 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:12 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2025-04-13 06:11:12 WARN  BlockStoragePolicy:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2025-04-13 06:11:12 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2025-04-13 06:11:12 INFO  StateChange:801 - BLOCK* allocate blk_1073741829_1005, replicas=172.19.0.4:9970, 172.19.0.5:9972 for /data/message_15d56695-89df-48ee-88a1-984f5f91e175
2025-04-13 06:11:12 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.5:9972 is added to blk_1073741829_1005 (size=72)
2025-04-13 06:11:12 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741829_1005 (size=72)
2025-04-13 06:11:12 INFO  StateChange:3239 - DIR* completeFile: /data/message_15d56695-89df-48ee-88a1-984f5f91e175 is closed by DFSClient_NONMAPREDUCE_-1706211797_1
2025-04-13 06:11:12 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_176302e4-3713-411f-894d-2829760149a3	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:11:12 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:12 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:12 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:12 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2025-04-13 06:11:12 WARN  BlockStoragePolicy:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2025-04-13 06:11:12 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2025-04-13 06:11:12 INFO  StateChange:801 - BLOCK* allocate blk_1073741830_1006, replicas=172.19.0.5:9972, 172.19.0.4:9970 for /data/message_176302e4-3713-411f-894d-2829760149a3
2025-04-13 06:11:12 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741830_1006 (size=72)
2025-04-13 06:11:12 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.5:9972 is added to blk_1073741830_1006 (size=72)
2025-04-13 06:11:12 INFO  StateChange:3239 - DIR* completeFile: /data/message_176302e4-3713-411f-894d-2829760149a3 is closed by DFSClient_NONMAPREDUCE_-1706211797_1
2025-04-13 06:11:12 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_f07b192b-0b87-4f87-8ced-1a82d9790c4b	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:11:12 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:12 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:12 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1, NODE_TOO_BUSY=1}
2025-04-13 06:11:12 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2025-04-13 06:11:12 WARN  BlockStoragePolicy:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2025-04-13 06:11:12 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2025-04-13 06:11:12 INFO  StateChange:801 - BLOCK* allocate blk_1073741831_1007, replicas=172.19.0.5:9972, 172.19.0.4:9970 for /data/message_f07b192b-0b87-4f87-8ced-1a82d9790c4b
2025-04-13 06:11:12 INFO  FSNamesystem:3286 - BLOCK* blk_1073741831_1007 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /data/message_f07b192b-0b87-4f87-8ced-1a82d9790c4b
2025-04-13 06:11:12 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741831_1007 (size=72)
2025-04-13 06:11:12 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.5:9972 is added to blk_1073741831_1007 (size=72)
2025-04-13 06:11:13 INFO  StateChange:3239 - DIR* completeFile: /data/message_f07b192b-0b87-4f87-8ced-1a82d9790c4b is closed by DFSClient_NONMAPREDUCE_-1706211797_1
2025-04-13 06:11:13 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_89545f63-e67e-440b-9231-8ee237b4e774	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2025-04-13 06:11:13 WARN  BlockStoragePolicy:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2025-04-13 06:11:13 INFO  StateChange:801 - BLOCK* allocate blk_1073741832_1008, replicas=172.19.0.5:9972, 172.19.0.4:9970 for /data/message_89545f63-e67e-440b-9231-8ee237b4e774
2025-04-13 06:11:13 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741832_1008 (size=72)
2025-04-13 06:11:13 INFO  StateChange:3239 - DIR* completeFile: /data/message_89545f63-e67e-440b-9231-8ee237b4e774 is closed by DFSClient_NONMAPREDUCE_-1706211797_1
2025-04-13 06:11:13 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_ef0f3dc9-5540-4ac7-bd78-88757638d04e	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:11:13 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.5:9972 is added to blk_1073741832_1008 (size=72)
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2025-04-13 06:11:13 WARN  BlockStoragePolicy:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2025-04-13 06:11:13 INFO  StateChange:801 - BLOCK* allocate blk_1073741833_1009, replicas=172.19.0.4:9970, 172.19.0.5:9972 for /data/message_ef0f3dc9-5540-4ac7-bd78-88757638d04e
2025-04-13 06:11:13 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.5:9972 is added to blk_1073741833_1009 (size=72)
2025-04-13 06:11:13 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741833_1009 (size=72)
2025-04-13 06:11:13 INFO  StateChange:3239 - DIR* completeFile: /data/message_ef0f3dc9-5540-4ac7-bd78-88757638d04e is closed by DFSClient_NONMAPREDUCE_-1706211797_1
2025-04-13 06:11:13 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_a2aa9d69-33db-45c5-a6fb-ead340156c40	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2025-04-13 06:11:13 WARN  BlockStoragePolicy:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2025-04-13 06:11:13 INFO  StateChange:801 - BLOCK* allocate blk_1073741834_1010, replicas=172.19.0.5:9972, 172.19.0.4:9970 for /data/message_a2aa9d69-33db-45c5-a6fb-ead340156c40
2025-04-13 06:11:13 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741834_1010 (size=72)
2025-04-13 06:11:13 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.5:9972 is added to blk_1073741834_1010 (size=72)
2025-04-13 06:11:13 INFO  StateChange:3239 - DIR* completeFile: /data/message_a2aa9d69-33db-45c5-a6fb-ead340156c40 is closed by DFSClient_NONMAPREDUCE_-1706211797_1
2025-04-13 06:11:13 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_fe873d73-32e3-4d10-834c-5f951272070f	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2025-04-13 06:11:13 WARN  BlockStoragePolicy:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2025-04-13 06:11:13 INFO  StateChange:801 - BLOCK* allocate blk_1073741835_1011, replicas=172.19.0.5:9972, 172.19.0.4:9970 for /data/message_fe873d73-32e3-4d10-834c-5f951272070f
2025-04-13 06:11:13 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741835_1011 (size=72)
2025-04-13 06:11:13 INFO  StateChange:3239 - DIR* completeFile: /data/message_fe873d73-32e3-4d10-834c-5f951272070f is closed by DFSClient_NONMAPREDUCE_-1706211797_1
2025-04-13 06:11:13 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.5:9972 is added to blk_1073741835_1011 (size=72)
2025-04-13 06:11:13 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_1db1bcf4-3aa3-4c85-aeba-a8fa5cf5cb5f	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2025-04-13 06:11:13 WARN  BlockStoragePolicy:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2025-04-13 06:11:13 INFO  StateChange:801 - BLOCK* allocate blk_1073741836_1012, replicas=172.19.0.4:9970, 172.19.0.5:9972 for /data/message_1db1bcf4-3aa3-4c85-aeba-a8fa5cf5cb5f
2025-04-13 06:11:13 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.5:9972 is added to blk_1073741836_1012 (size=72)
2025-04-13 06:11:13 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741836_1012 (size=72)
2025-04-13 06:11:13 INFO  StateChange:3239 - DIR* completeFile: /data/message_1db1bcf4-3aa3-4c85-aeba-a8fa5cf5cb5f is closed by DFSClient_NONMAPREDUCE_-1706211797_1
2025-04-13 06:11:13 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_d585ead6-fedd-455a-9d69-6c996ad6f6b2	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2025-04-13 06:11:13 WARN  BlockStoragePolicy:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2025-04-13 06:11:13 INFO  StateChange:801 - BLOCK* allocate blk_1073741837_1013, replicas=172.19.0.5:9972, 172.19.0.4:9970 for /data/message_d585ead6-fedd-455a-9d69-6c996ad6f6b2
2025-04-13 06:11:13 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741837_1013 (size=72)
2025-04-13 06:11:13 INFO  StateChange:3239 - DIR* completeFile: /data/message_d585ead6-fedd-455a-9d69-6c996ad6f6b2 is closed by DFSClient_NONMAPREDUCE_-1706211797_1
2025-04-13 06:11:13 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.5:9972 is added to blk_1073741837_1013 (size=72)
2025-04-13 06:11:13 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_81a94a95-efd3-48c2-9525-cb64afdac984	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2025-04-13 06:11:13 WARN  BlockStoragePolicy:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2025-04-13 06:11:13 INFO  StateChange:801 - BLOCK* allocate blk_1073741838_1014, replicas=172.19.0.5:9972, 172.19.0.4:9970 for /data/message_81a94a95-efd3-48c2-9525-cb64afdac984
2025-04-13 06:11:13 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741838_1014 (size=72)
2025-04-13 06:11:13 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.5:9972 is added to blk_1073741838_1014 (size=72)
2025-04-13 06:11:13 INFO  StateChange:3239 - DIR* completeFile: /data/message_81a94a95-efd3-48c2-9525-cb64afdac984 is closed by DFSClient_NONMAPREDUCE_-1706211797_1
2025-04-13 06:11:13 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_3078c3c0-8bb2-4361-abc1-c67ee777f9f8	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2025-04-13 06:11:13 WARN  BlockStoragePolicy:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2025-04-13 06:11:13 INFO  StateChange:801 - BLOCK* allocate blk_1073741839_1015, replicas=172.19.0.4:9970, 172.19.0.5:9972 for /data/message_3078c3c0-8bb2-4361-abc1-c67ee777f9f8
2025-04-13 06:11:13 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.5:9972 is added to blk_1073741839_1015 (size=72)
2025-04-13 06:11:13 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741839_1015 (size=72)
2025-04-13 06:11:13 INFO  StateChange:3239 - DIR* completeFile: /data/message_3078c3c0-8bb2-4361-abc1-c67ee777f9f8 is closed by DFSClient_NONMAPREDUCE_-1706211797_1
2025-04-13 06:11:13 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_300e36b0-9092-4823-86f0-08daee5cc5b8	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2025-04-13 06:11:13 WARN  BlockStoragePolicy:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2025-04-13 06:11:13 INFO  StateChange:801 - BLOCK* allocate blk_1073741840_1016, replicas=172.19.0.5:9972, 172.19.0.4:9970 for /data/message_300e36b0-9092-4823-86f0-08daee5cc5b8
2025-04-13 06:11:13 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741840_1016 (size=72)
2025-04-13 06:11:13 INFO  StateChange:3239 - DIR* completeFile: /data/message_300e36b0-9092-4823-86f0-08daee5cc5b8 is closed by DFSClient_NONMAPREDUCE_-1706211797_1
2025-04-13 06:11:13 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.5:9972 is added to blk_1073741840_1016 (size=72)
2025-04-13 06:11:13 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_efc296cb-bb6f-4f65-839c-78ec52e6ae34	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2025-04-13 06:11:13 WARN  BlockStoragePolicy:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2025-04-13 06:11:13 INFO  StateChange:801 - BLOCK* allocate blk_1073741841_1017, replicas=172.19.0.4:9970, 172.19.0.5:9972 for /data/message_efc296cb-bb6f-4f65-839c-78ec52e6ae34
2025-04-13 06:11:13 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.5:9972 is added to blk_1073741841_1017 (size=69)
2025-04-13 06:11:13 INFO  StateChange:3239 - DIR* completeFile: /data/message_efc296cb-bb6f-4f65-839c-78ec52e6ae34 is closed by DFSClient_NONMAPREDUCE_-1706211797_1
2025-04-13 06:11:13 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741841_1017 (size=69)
2025-04-13 06:11:13 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_76f5df34-bca2-45f0-817f-48cb492ffcc6	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1, NODE_TOO_BUSY=1}
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2025-04-13 06:11:13 WARN  BlockStoragePolicy:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2025-04-13 06:11:13 INFO  StateChange:801 - BLOCK* allocate blk_1073741842_1018, replicas=172.19.0.4:9970, 172.19.0.5:9972 for /data/message_76f5df34-bca2-45f0-817f-48cb492ffcc6
2025-04-13 06:11:13 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741842_1018 (size=72)
2025-04-13 06:11:13 INFO  StateChange:3239 - DIR* completeFile: /data/message_76f5df34-bca2-45f0-817f-48cb492ffcc6 is closed by DFSClient_NONMAPREDUCE_-1706211797_1
2025-04-13 06:11:13 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.5:9972 is added to blk_1073741842_1018 (size=72)
2025-04-13 06:11:13 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_71dfdfb9-89c2-4b6b-b8f3-073030a3aed8	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2025-04-13 06:11:13 WARN  BlockStoragePolicy:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2025-04-13 06:11:13 INFO  StateChange:801 - BLOCK* allocate blk_1073741843_1019, replicas=172.19.0.4:9970, 172.19.0.5:9972 for /data/message_71dfdfb9-89c2-4b6b-b8f3-073030a3aed8
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1, NODE_TOO_BUSY=1}
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2025-04-13 06:11:13 WARN  BlockStoragePolicy:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK, ARCHIVE], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) All required storage types are unavailable:  unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1, NODE_TOO_BUSY=1}
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2025-04-13 06:11:13 WARN  BlockStoragePolicy:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK, ARCHIVE], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) All required storage types are unavailable:  unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1, NODE_TOO_BUSY=1}
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2025-04-13 06:11:13 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.5:9972 is added to blk_1073741843_1019 (size=72)
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2025-04-13 06:11:13 WARN  BlockStoragePolicy:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK, ARCHIVE], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) All required storage types are unavailable:  unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2025-04-13 06:11:13 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741843_1019 (size=72)
2025-04-13 06:11:13 INFO  StateChange:3239 - DIR* completeFile: /data/message_71dfdfb9-89c2-4b6b-b8f3-073030a3aed8 is closed by DFSClient_NONMAPREDUCE_-1706211797_1
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1, NODE_TOO_BUSY=1}
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2025-04-13 06:11:13 WARN  BlockStoragePolicy:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK, ARCHIVE], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) All required storage types are unavailable:  unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2025-04-13 06:11:13 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_7d0be6fa-ec0a-439b-9a96-e74f74afcca4	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2025-04-13 06:11:13 WARN  BlockStoragePolicy:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2025-04-13 06:11:13 WARN  BlockPlacementPolicy:501 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2025-04-13 06:11:13 INFO  StateChange:801 - BLOCK* allocate blk_1073741844_1020, replicas=172.19.0.4:9970, 172.19.0.5:9972 for /data/message_7d0be6fa-ec0a-439b-9a96-e74f74afcca4
2025-04-13 06:11:13 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.5:9972 is added to blk_1073741844_1020 (size=69)
2025-04-13 06:11:13 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741844_1020 (size=69)
2025-04-13 06:11:13 INFO  StateChange:3239 - DIR* completeFile: /data/message_7d0be6fa-ec0a-439b-9a96-e74f74afcca4 is closed by DFSClient_NONMAPREDUCE_-1706211797_1
2025-04-13 06:11:16 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_9ed3d4e2-c8f5-4ae7-bb31-f02cee72a954	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:11:16 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:16 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:16 INFO  StateChange:801 - BLOCK* allocate blk_1073741845_1021, replicas=172.19.0.3:9971, 172.19.0.4:9970, 172.19.0.5:9972 for /data/message_9ed3d4e2-c8f5-4ae7-bb31-f02cee72a954
2025-04-13 06:11:16 INFO  FSNamesystem:3286 - BLOCK* blk_1073741845_1021 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /data/message_9ed3d4e2-c8f5-4ae7-bb31-f02cee72a954
2025-04-13 06:11:16 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741845_1021 (size=72)
2025-04-13 06:11:16 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.5:9972 is added to blk_1073741845_1021 (size=72)
2025-04-13 06:11:16 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741845_1021 (size=72)
2025-04-13 06:11:16 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:16 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:16 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:16 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:17 INFO  StateChange:3239 - DIR* completeFile: /data/message_9ed3d4e2-c8f5-4ae7-bb31-f02cee72a954 is closed by DFSClient_NONMAPREDUCE_-1706211797_1
2025-04-13 06:11:17 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741832_1008 (size=72)
2025-04-13 06:11:17 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741834_1010 (size=72)
2025-04-13 06:11:17 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741833_1009 (size=72)
2025-04-13 06:11:17 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741835_1011 (size=72)
2025-04-13 06:11:19 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_4cd5abad-4374-40dc-9077-8f28150c1c85	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:11:19 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:19 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:19 INFO  StateChange:801 - BLOCK* allocate blk_1073741846_1022, replicas=172.19.0.3:9971, 172.19.0.4:9970, 172.19.0.5:9972 for /data/message_4cd5abad-4374-40dc-9077-8f28150c1c85
2025-04-13 06:11:19 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.5:9972 is added to blk_1073741846_1022 (size=72)
2025-04-13 06:11:19 INFO  StateChange:3239 - DIR* completeFile: /data/message_4cd5abad-4374-40dc-9077-8f28150c1c85 is closed by DFSClient_NONMAPREDUCE_-1706211797_1
2025-04-13 06:11:19 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741846_1022 (size=72)
2025-04-13 06:11:19 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741846_1022 (size=72)
2025-04-13 06:11:19 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:19 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:19 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:19 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:20 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741840_1016 (size=72)
2025-04-13 06:11:20 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741841_1017 (size=69)
2025-04-13 06:11:20 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741838_1014 (size=72)
2025-04-13 06:11:20 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741839_1015 (size=72)
2025-04-13 06:11:22 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_1022cc44-f5bf-43c8-bbfe-7c59838e664a	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:11:22 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:22 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:22 INFO  StateChange:801 - BLOCK* allocate blk_1073741847_1023, replicas=172.19.0.3:9971, 172.19.0.4:9970, 172.19.0.5:9972 for /data/message_1022cc44-f5bf-43c8-bbfe-7c59838e664a
2025-04-13 06:11:22 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.5:9972 is added to blk_1073741847_1023 (size=72)
2025-04-13 06:11:22 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741847_1023 (size=72)
2025-04-13 06:11:22 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741847_1023 (size=72)
2025-04-13 06:11:22 INFO  StateChange:3239 - DIR* completeFile: /data/message_1022cc44-f5bf-43c8-bbfe-7c59838e664a is closed by DFSClient_NONMAPREDUCE_-1706211797_1
2025-04-13 06:11:22 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:23 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741844_1020 (size=69)
2025-04-13 06:11:25 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_f19c279b-fde2-467f-b4d9-5dae632bff0b	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:11:25 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:25 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:25 INFO  StateChange:801 - BLOCK* allocate blk_1073741848_1024, replicas=172.19.0.5:9972, 172.19.0.3:9971, 172.19.0.4:9970 for /data/message_f19c279b-fde2-467f-b4d9-5dae632bff0b
2025-04-13 06:11:25 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741848_1024 (size=72)
2025-04-13 06:11:25 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741848_1024 (size=72)
2025-04-13 06:11:25 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.5:9972 is added to blk_1073741848_1024 (size=72)
2025-04-13 06:11:25 INFO  StateChange:3239 - DIR* completeFile: /data/message_f19c279b-fde2-467f-b4d9-5dae632bff0b is closed by DFSClient_NONMAPREDUCE_-1706211797_1
2025-04-13 06:11:25 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:25 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:25 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:25 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:26 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741826_1002 (size=69)
2025-04-13 06:11:26 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741829_1005 (size=72)
2025-04-13 06:11:26 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741827_1003 (size=72)
2025-04-13 06:11:26 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741828_1004 (size=72)
2025-04-13 06:11:28 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_9065f804-f3e6-495d-b101-9747eed2dbcf	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:11:28 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:28 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:28 INFO  StateChange:801 - BLOCK* allocate blk_1073741849_1025, replicas=172.19.0.3:9971, 172.19.0.4:9970, 172.19.0.5:9972 for /data/message_9065f804-f3e6-495d-b101-9747eed2dbcf
2025-04-13 06:11:28 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.5:9972 is added to blk_1073741849_1025 (size=72)
2025-04-13 06:11:28 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741849_1025 (size=72)
2025-04-13 06:11:28 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741849_1025 (size=72)
2025-04-13 06:11:28 INFO  StateChange:3239 - DIR* completeFile: /data/message_9065f804-f3e6-495d-b101-9747eed2dbcf is closed by DFSClient_NONMAPREDUCE_-1706211797_1
2025-04-13 06:11:28 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:28 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:28 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:28 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:29 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741837_1013 (size=72)
2025-04-13 06:11:29 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741836_1012 (size=72)
2025-04-13 06:11:29 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741843_1019 (size=72)
2025-04-13 06:11:29 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741842_1018 (size=72)
2025-04-13 06:11:31 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_2c644a19-aacb-490e-9354-8c815f84adbe	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:11:31 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:31 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:31 INFO  StateChange:801 - BLOCK* allocate blk_1073741850_1026, replicas=172.19.0.3:9971, 172.19.0.5:9972, 172.19.0.4:9970 for /data/message_2c644a19-aacb-490e-9354-8c815f84adbe
2025-04-13 06:11:31 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741850_1026 (size=72)
2025-04-13 06:11:31 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.5:9972 is added to blk_1073741850_1026 (size=72)
2025-04-13 06:11:31 INFO  StateChange:3239 - DIR* completeFile: /data/message_2c644a19-aacb-490e-9354-8c815f84adbe is closed by DFSClient_NONMAPREDUCE_-1706211797_1
2025-04-13 06:11:31 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741850_1026 (size=72)
2025-04-13 06:11:31 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:31 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:32 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741831_1007 (size=72)
2025-04-13 06:11:32 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741830_1006 (size=72)
2025-04-13 06:11:34 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_53875f9b-c89f-4c35-971c-668b0150de66	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:11:34 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:34 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:34 INFO  StateChange:801 - BLOCK* allocate blk_1073741851_1027, replicas=172.19.0.3:9971, 172.19.0.5:9972, 172.19.0.4:9970 for /data/message_53875f9b-c89f-4c35-971c-668b0150de66
2025-04-13 06:11:34 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.5:9972 is added to blk_1073741851_1027 (size=72)
2025-04-13 06:11:34 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741851_1027 (size=72)
2025-04-13 06:11:34 INFO  StateChange:3239 - DIR* completeFile: /data/message_53875f9b-c89f-4c35-971c-668b0150de66 is closed by DFSClient_NONMAPREDUCE_-1706211797_1
2025-04-13 06:11:34 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741851_1027 (size=72)
2025-04-13 06:11:37 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_01c1ddef-2f8a-4679-8b84-847e566b292a	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:11:37 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:37 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:37 INFO  StateChange:801 - BLOCK* allocate blk_1073741852_1028, replicas=172.19.0.4:9970, 172.19.0.5:9972, 172.19.0.3:9971 for /data/message_01c1ddef-2f8a-4679-8b84-847e566b292a
2025-04-13 06:11:37 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741852_1028 (size=72)
2025-04-13 06:11:37 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.5:9972 is added to blk_1073741852_1028 (size=72)
2025-04-13 06:11:37 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741852_1028 (size=72)
2025-04-13 06:11:37 INFO  StateChange:3239 - DIR* completeFile: /data/message_01c1ddef-2f8a-4679-8b84-847e566b292a is closed by DFSClient_NONMAPREDUCE_-1706211797_1
2025-04-13 06:11:40 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_0c759f87-39b5-4063-a1eb-317edf93a3f1	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:11:40 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:40 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:40 INFO  StateChange:801 - BLOCK* allocate blk_1073741853_1029, replicas=172.19.0.4:9970, 172.19.0.3:9971, 172.19.0.5:9972 for /data/message_0c759f87-39b5-4063-a1eb-317edf93a3f1
2025-04-13 06:11:40 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.5:9972 is added to blk_1073741853_1029 (size=72)
2025-04-13 06:11:40 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741853_1029 (size=72)
2025-04-13 06:11:40 INFO  StateChange:3239 - DIR* completeFile: /data/message_0c759f87-39b5-4063-a1eb-317edf93a3f1 is closed by DFSClient_NONMAPREDUCE_-1706211797_1
2025-04-13 06:11:40 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741853_1029 (size=72)
2025-04-13 06:11:43 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_ac7d5ff1-215b-40ff-a6d4-74cec3c51ec2	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:11:43 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:43 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:11:43 INFO  StateChange:801 - BLOCK* allocate blk_1073741854_1030, replicas=172.19.0.3:9971, 172.19.0.4:9970, 172.19.0.5:9972 for /data/message_ac7d5ff1-215b-40ff-a6d4-74cec3c51ec2
2025-04-13 06:11:43 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.5:9972 is added to blk_1073741854_1030 (size=72)
2025-04-13 06:11:43 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741854_1030 (size=72)
2025-04-13 06:11:43 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741854_1030 (size=72)
2025-04-13 06:11:43 INFO  StateChange:3239 - DIR* completeFile: /data/message_ac7d5ff1-215b-40ff-a6d4-74cec3c51ec2 is closed by DFSClient_NONMAPREDUCE_-1706211797_1
2025-04-13 06:35:23 INFO  NameNode:809 - STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = hadoop-namenode/172.19.0.5
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.4.1
STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/netty-common-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/kerby-asn1-2.0.3.jar:/opt/hadoop/share/hadoop/common/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.100.Final-linux-x86_64.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.100.Final-linux-aarch_64.jar:/opt/hadoop/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-security-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-mqtt-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-haproxy-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-http2-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.9.0.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-dns-classes-macos-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-3.4.1.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.9.2.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-http-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/checker-qual-2.5.2.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-classes-kqueue-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-redis-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-xml-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.55.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-dns-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-5.2.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.9.0.jar:/opt/hadoop/share/hadoop/common/lib/kerb-core-2.0.3.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-aarch_64.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration2-2.10.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.26.1.jar:/opt/hadoop/share/hadoop/common/lib/bcprov-jdk18on-1.78.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/common/lib/jetty-http-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/common/lib/kerby-pkix-2.0.3.jar:/opt/hadoop/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/hadoop/common/lib/dnsjava-3.6.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-unix-common-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.19.4.jar:/opt/hadoop/share/hadoop/common/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang3-3.12.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-jute-3.8.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.5.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-servlet-1.19.4.jar:/opt/hadoop/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop/share/hadoop/common/lib/jetty-xml-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/common/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-buffer-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-handler-ssl-ocsp-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/jetty-server-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.19.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.6.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-5.2.0.jar:/opt/hadoop/share/hadoop/common/lib/netty-handler-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.13.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.100.Final-osx-x86_64.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-ajax-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-udt-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-dns-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-sctp-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/kerby-util-2.0.3.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.2.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.8.4.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-smtp-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-5.2.0.jar:/opt/hadoop/share/hadoop/common/lib/failureaccess-1.0.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-classes-epoll-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.22.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.15.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.1.10.4.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-rxtx-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-socks-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.16.1.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/re2j-1.1.jar:/opt/hadoop/share/hadoop/common/lib/kerb-crypto-2.0.3.jar:/opt/hadoop/share/hadoop/common/lib/jetty-servlet-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-2.12.7.jar:/opt/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar:/opt/hadoop/share/hadoop/common/lib/reload4j-1.2.22.jar:/opt/hadoop/share/hadoop/common/lib/jackson-databind-2.12.7.1.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-9.37.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-text-1.10.0.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-3.4.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-memcache-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.5.4.jar:/opt/hadoop/share/hadoop/common/lib/kerby-config-2.0.3.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.13.jar:/opt/hadoop/share/hadoop/common/lib/netty-handler-proxy-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-stomp-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/jul-to-slf4j-1.7.36.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/kerb-util-2.0.3.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-x86_64.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-shaded-protobuf_3_25-1.3.0.jar:/opt/hadoop/share/hadoop/common/lib/audience-annotations-0.12.0.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.2.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.100.Final-osx-aarch_64.jar:/opt/hadoop/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-io-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/common/lib/netty-all-4.1.100.Final.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-shaded-guava-1.3.0.jar:/opt/hadoop/share/hadoop/common/lib/jackson-annotations-2.12.7.jar:/opt/hadoop/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/hadoop/common/lib/jetty-webapp-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-3.4.1.jar:/opt/hadoop/share/hadoop/common/hadoop-common-3.4.1.jar:/opt/hadoop/share/hadoop/common/hadoop-common-3.4.1-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-registry-3.4.1.jar:/opt/hadoop/share/hadoop/common/hadoop-kms-3.4.1.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/netty-common-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-asn1-2.0.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.100.Final-linux-x86_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.100.Final-linux-aarch_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-security-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-mqtt-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-haproxy-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-http2-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/gson-2.9.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-classes-macos-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-auth-3.4.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/avro-1.9.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-http-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-classes-kqueue-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-redis-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-xml-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-dns-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-client-5.2.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-net-3.9.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-core-2.0.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-aarch_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-configuration2-2.10.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-compress-1.26.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-http-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-pkix-2.0.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/hadoop/hdfs/lib/dnsjava-3.6.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-unix-common-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.19.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang3-3.12.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/zookeeper-jute-3.8.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-servlet-1.19.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-xml-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-buffer-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-handler-ssl-ocsp-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-server-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.19.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-math3-3.6.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-recipes-5.2.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/HikariCP-4.0.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-handler-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.100.Final-osx-x86_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-udt-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-sctp-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-util-2.0.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/zookeeper-3.8.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-smtp-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-framework-5.2.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-classes-epoll-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-json-1.22.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.15.jar:/opt/hadoop/share/hadoop/hdfs/lib/snappy-java-1.1.10.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-rxtx-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-socks-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.16.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/hdfs/lib/re2j-1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-crypto-2.0.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-servlet-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-2.12.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/opt/hadoop/share/hadoop/hdfs/lib/reload4j-1.2.22.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-databind-2.12.7.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.37.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-text-1.10.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-annotations-3.4.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-memcache-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jettison-1.5.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-config-2.0.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-handler-proxy-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-stomp-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-util-2.0.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-x86_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_25-1.3.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/audience-annotations-0.12.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.100.Final-osx-aarch_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-io-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.1.100.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-shaded-guava-1.3.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-annotations-2.12.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-webapp-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.4.1.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.4.1.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.4.1-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.4.1-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.4.1-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.4.1-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-3.4.1.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.4.1.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.4.1.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.4.1-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.4.1.jar:/opt/hadoop/share/hadoop/yarn:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.12.7.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-jndi-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/yarn/lib/javax-websocket-server-impl-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-api-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.websocket-client-api-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/cache-api-1.1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-server-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-base-2.12.7.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-annotations-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/yarn/lib/javax-websocket-client-impl-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/yarn/lib/ehcache-3.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/FastInfoset-1.2.15.jar:/opt/hadoop/share/hadoop/yarn/lib/snakeyaml-2.0.jar:/opt/hadoop/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-servlet-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-commons-9.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-4.2.3.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.19.4.jar:/opt/hadoop/share/hadoop/yarn/lib/jna-5.2.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-tree-9.6.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-plus-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/yarn/lib/istack-commons-runtime-3.0.7.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-ex-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/txw2-2.3.1.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-client-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-client-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/yarn/lib/bcutil-jdk18on-1.78.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jline-3.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/codemodel-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.websocket-api-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-4.2.3.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.12.7.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.19.4.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-runtime-2.3.1.jar:/opt/hadoop/share/hadoop/yarn/lib/bcpkix-jdk18on-1.78.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsonschema2pojo-core-1.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/hadoop/yarn/lib/objenesis-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-common-9.4.53.v20231009.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-services-core-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-mawo-core-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-services-api-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-globalpolicygenerator-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-router-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-3.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-3.4.1.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r 4d7825309348956336b8f06a08322b78422849b1; compiled by 'mthakur' on 2024-10-09T14:57Z
STARTUP_MSG:   java = 1.8.0_212
************************************************************/
2025-04-13 06:35:23 INFO  NameNode:91 - registered UNIX signal handlers for [TERM, HUP, INT]
2025-04-13 06:35:24 INFO  NameNode:1846 - createNameNode []
2025-04-13 06:35:25 INFO  MetricsConfig:122 - Loaded properties from hadoop-metrics2.properties
2025-04-13 06:35:26 INFO  MetricsSystemImpl:378 - Scheduled Metric snapshot period at 10 second(s).
2025-04-13 06:35:26 INFO  MetricsSystemImpl:191 - NameNode metrics system started
2025-04-13 06:35:26 INFO  NameNodeUtils:79 - fs.defaultFS is hdfs://hadoop-namenode:9000
2025-04-13 06:35:26 INFO  NameNode:1149 - Clients should use hadoop-namenode:9000 to access this namenode/service.
2025-04-13 06:35:28 INFO  JvmPauseMonitor:185 - Starting JVM pause monitor
2025-04-13 06:35:28 INFO  DFSUtil:1769 - Filter initializers set : org.apache.hadoop.http.lib.StaticUserWebFilter,org.apache.hadoop.hdfs.web.AuthFilterInitializer
2025-04-13 06:35:28 INFO  DFSUtil:1791 - Starting Web-server for hdfs at: http://0.0.0.0:9870
2025-04-13 06:35:28 INFO  log:170 - Logging initialized @9974ms to org.eclipse.jetty.util.log.Slf4jLog
2025-04-13 06:35:29 WARN  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
2025-04-13 06:35:30 INFO  HttpServer2:1205 - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2025-04-13 06:35:30 INFO  HttpServer2:1178 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2025-04-13 06:35:30 INFO  HttpServer2:1188 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2025-04-13 06:35:30 INFO  HttpServer2:1188 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2025-04-13 06:35:30 INFO  HttpServer2:1178 - Added filter AuthFilter (class=org.apache.hadoop.hdfs.web.AuthFilter) to context hdfs
2025-04-13 06:35:30 INFO  HttpServer2:1188 - Added filter AuthFilter (class=org.apache.hadoop.hdfs.web.AuthFilter) to context logs
2025-04-13 06:35:30 INFO  HttpServer2:1188 - Added filter AuthFilter (class=org.apache.hadoop.hdfs.web.AuthFilter) to context static
2025-04-13 06:35:30 INFO  HttpServer2:813 - ASYNC_PROFILER_HOME environment variable and async.profiler.home system property not specified. Disabling /prof endpoint.
2025-04-13 06:35:30 INFO  HttpServer2:1032 - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2025-04-13 06:35:30 INFO  HttpServer2:1452 - Jetty bound to port 9870
2025-04-13 06:35:30 INFO  Server:375 - jetty-9.4.53.v20231009; built: 2023-10-09T12:29:09.265Z; git: 27bde00a0b95a1d5bbee0eae7984f891d2d0f8c9; jvm 1.8.0_212-b04
2025-04-13 06:35:31 INFO  session:334 - DefaultSessionIdManager workerName=node0
2025-04-13 06:35:31 INFO  session:339 - No SessionScavenger set, using defaults
2025-04-13 06:35:31 INFO  session:132 - node0 Scavenging every 600000ms
2025-04-13 06:35:31 WARN  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
2025-04-13 06:35:31 INFO  ContextHandler:921 - Started o.e.j.s.ServletContextHandler@9816741{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
2025-04-13 06:35:31 INFO  ContextHandler:921 - Started o.e.j.s.ServletContextHandler@7dc3712{static,/static,file:///opt/hadoop/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2025-04-13 06:35:31 INFO  ContextHandler:921 - Started o.e.j.w.WebAppContext@20c0a64d{hdfs,/,file:///opt/hadoop/share/hadoop/hdfs/webapps/hdfs/,AVAILABLE}{file:/opt/hadoop/share/hadoop/hdfs/webapps/hdfs}
2025-04-13 06:35:31 INFO  AbstractConnector:333 - Started ServerConnector@6ca18a14{HTTP/1.1, (http/1.1)}{0.0.0.0:9870}
2025-04-13 06:35:32 INFO  Server:415 - Started @13397ms
2025-04-13 06:35:33 WARN  FSNamesystem:800 - Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2025-04-13 06:35:33 WARN  FSNamesystem:805 - Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2025-04-13 06:35:33 INFO  FSEditLog:238 - Edit logging is async:true
2025-04-13 06:35:34 INFO  FSNamesystem:869 - KeyProvider: null
2025-04-13 06:35:34 INFO  FSNamesystem:142 - fsLock is fair: true
2025-04-13 06:35:34 INFO  FSNamesystem:160 - Detailed lock hold time metrics enabled: false
2025-04-13 06:35:34 INFO  FSNamesystem:904 - fsOwner                = hadoop (auth:SIMPLE)
2025-04-13 06:35:34 INFO  FSNamesystem:905 - supergroup             = supergroup
2025-04-13 06:35:34 INFO  FSNamesystem:906 - isPermissionEnabled    = true
2025-04-13 06:35:34 INFO  FSNamesystem:907 - isStoragePolicyEnabled = true
2025-04-13 06:35:34 INFO  FSNamesystem:918 - HA Enabled: false
2025-04-13 06:35:34 INFO  Util:428 - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2025-04-13 06:35:35 INFO  DatanodeManager:412 - Slow peers collection thread shutdown
2025-04-13 06:35:36 INFO  DatanodeManager:2182 - dfs.block.invalidate.limit : configured=1000, counted=60, effected=1000
2025-04-13 06:35:36 INFO  DatanodeManager:320 - dfs.namenode.datanode.registration.ip-hostname-check=true
2025-04-13 06:35:36 INFO  BlockManager:77 - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2025-04-13 06:35:36 INFO  BlockManager:83 - The block deletion will start around 2025 Apr 13 06:35:36
2025-04-13 06:35:36 INFO  GSet:408 - Computing capacity for map BlocksMap
2025-04-13 06:35:36 INFO  GSet:409 - VM type       = 64-bit
2025-04-13 06:35:36 INFO  GSet:410 - 2.0% max memory 1.0 GB = 21.3 MB
2025-04-13 06:35:36 INFO  GSet:415 - capacity      = 2^21 = 2097152 entries
2025-04-13 06:35:36 INFO  BlockManager:5778 - Storage policy satisfier is disabled
2025-04-13 06:35:36 INFO  BlockManager:696 - dfs.block.access.token.enable = false
2025-04-13 06:35:36 INFO  BlockManagerSafeMode:656 - Using 1000 as SafeModeMonitor Interval
2025-04-13 06:35:36 INFO  BlockManagerSafeMode:161 - dfs.namenode.safemode.threshold-pct = 0.999
2025-04-13 06:35:36 INFO  BlockManagerSafeMode:162 - dfs.namenode.safemode.min.datanodes = 0
2025-04-13 06:35:36 INFO  BlockManagerSafeMode:164 - dfs.namenode.safemode.extension = 30000
2025-04-13 06:35:36 INFO  BlockManager:682 - defaultReplication         = 3
2025-04-13 06:35:36 INFO  BlockManager:683 - maxReplication             = 512
2025-04-13 06:35:36 INFO  BlockManager:684 - minReplication             = 1
2025-04-13 06:35:36 INFO  BlockManager:685 - maxReplicationStreams      = 2
2025-04-13 06:35:36 INFO  BlockManager:686 - redundancyRecheckInterval  = 3000ms
2025-04-13 06:35:36 INFO  BlockManager:687 - encryptDataTransfer        = false
2025-04-13 06:35:36 INFO  BlockManager:688 - maxNumBlocksToLog          = 1000
2025-04-13 06:35:36 INFO  FSDirectory:51 - GLOBAL serial map: bits=29 maxEntries=536870911
2025-04-13 06:35:36 INFO  FSDirectory:51 - USER serial map: bits=24 maxEntries=16777215
2025-04-13 06:35:36 INFO  FSDirectory:51 - GROUP serial map: bits=24 maxEntries=16777215
2025-04-13 06:35:36 INFO  FSDirectory:51 - XATTR serial map: bits=24 maxEntries=16777215
2025-04-13 06:35:37 INFO  GSet:408 - Computing capacity for map INodeMap
2025-04-13 06:35:37 INFO  GSet:409 - VM type       = 64-bit
2025-04-13 06:35:37 INFO  GSet:410 - 1.0% max memory 1.0 GB = 10.7 MB
2025-04-13 06:35:37 INFO  GSet:415 - capacity      = 2^20 = 1048576 entries
2025-04-13 06:35:37 INFO  FSDirectory:339 - ACLs enabled? true
2025-04-13 06:35:37 INFO  FSDirectory:343 - POSIX ACL inheritance enabled? true
2025-04-13 06:35:37 INFO  FSDirectory:347 - XAttrs enabled? true
2025-04-13 06:35:37 INFO  NameNode:414 - Caching file names occurring more than 10 times
2025-04-13 06:35:37 INFO  SnapshotManager:163 - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotFSLimit: 65536, maxSnapshotLimit: 65536
2025-04-13 06:35:37 INFO  SnapshotManager:176 - dfs.namenode.snapshot.deletion.ordered = false
2025-04-13 06:35:37 INFO  SnapshotManager:43 - SkipList is disabled
2025-04-13 06:35:37 INFO  GSet:408 - Computing capacity for map cachedBlocks
2025-04-13 06:35:37 INFO  GSet:409 - VM type       = 64-bit
2025-04-13 06:35:37 INFO  GSet:410 - 0.25% max memory 1.0 GB = 2.7 MB
2025-04-13 06:35:37 INFO  GSet:415 - capacity      = 2^18 = 262144 entries
2025-04-13 06:35:37 INFO  TopMetrics:76 - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2025-04-13 06:35:37 INFO  TopMetrics:78 - NNTop conf: dfs.namenode.top.num.users = 10
2025-04-13 06:35:37 INFO  TopMetrics:80 - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2025-04-13 06:35:37 INFO  FSNamesystem:1158 - Retry cache on namenode is enabled
2025-04-13 06:35:37 INFO  FSNamesystem:1166 - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2025-04-13 06:35:37 INFO  GSet:408 - Computing capacity for map NameNodeRetryCache
2025-04-13 06:35:37 INFO  GSet:409 - VM type       = 64-bit
2025-04-13 06:35:37 INFO  GSet:410 - 0.029999999329447746% max memory 1.0 GB = 327.8 KB
2025-04-13 06:35:37 INFO  GSet:415 - capacity      = 2^15 = 32768 entries
2025-04-13 06:35:37 INFO  Storage:948 - Lock on /opt/hdfs/namenode/in_use.lock acquired by nodename 15@hadoop-namenode
2025-04-13 06:35:37 INFO  FileJournalManager:428 - Recovering unfinalized segments in /opt/hdfs/namenode/current
2025-04-13 06:35:37 INFO  FileJournalManager:145 - Finalizing edits file /opt/hdfs/namenode/current/edits_inprogress_0000000000000000001 -> /opt/hdfs/namenode/current/edits_0000000000000000001-0000000000000000152
2025-04-13 06:35:37 INFO  FSImage:800 - Planning to load image: FSImageFile(file=/opt/hdfs/namenode/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2025-04-13 06:35:38 INFO  FSImageFormatPBINode:411 - Loading 1 INodes.
2025-04-13 06:35:38 INFO  FSImageFormatPBINode:369 - Successfully loaded 1 inodes
2025-04-13 06:35:38 INFO  FSImageFormatPBINode:342 - Completed update blocks map and name cache, total waiting duration 0ms.
2025-04-13 06:35:39 INFO  FSImageFormatProtobuf:255 - Loaded FSImage in 0 seconds.
2025-04-13 06:35:39 INFO  FSImage:980 - Loaded image for txid 0 from /opt/hdfs/namenode/current/fsimage_0000000000000000000
2025-04-13 06:35:39 INFO  FSImage:913 - Reading /opt/hdfs/namenode/current/edits_0000000000000000001-0000000000000000152 expecting start txid #1
2025-04-13 06:35:39 INFO  FSImage:179 - Start loading edits file /opt/hdfs/namenode/current/edits_0000000000000000001-0000000000000000152 maxTxnsToRead = 9223372036854775807
2025-04-13 06:35:39 INFO  RedundantEditLogInputStream:187 - Fast-forwarding stream '/opt/hdfs/namenode/current/edits_0000000000000000001-0000000000000000152' to transaction ID 1
2025-04-13 06:35:39 INFO  FSImage:189 - Loaded 1 edits file(s) (the last named /opt/hdfs/namenode/current/edits_0000000000000000001-0000000000000000152) of total size 1048576.0, total edits 152.0, total load time 712.0 ms
2025-04-13 06:35:39 INFO  FSNamesystem:1280 - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2025-04-13 06:35:39 INFO  FSEditLog:1417 - Starting log segment at 153
2025-04-13 06:35:40 INFO  NameCache:143 - initialized with 0 entries 0 lookups
2025-04-13 06:35:40 INFO  FSNamesystem:841 - Finished loading FSImage in 2885 msecs
2025-04-13 06:35:41 INFO  NameNode:452 - RPC server is binding to hadoop-namenode:9000
2025-04-13 06:35:41 INFO  NameNode:457 - Enable NameNode state context:false
2025-04-13 06:35:41 INFO  CallQueueManager:93 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false, ipcFailOver: false.
2025-04-13 06:35:42 INFO  Server:1438 - Listener at hadoop-namenode:9000
2025-04-13 06:35:42 INFO  Server:1474 - Starting Socket Reader #1 for port 9000
2025-04-13 06:35:42 INFO  FSNamesystem:5656 - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2025-04-13 06:35:42 INFO  LeaseManager:166 - Number of blocks under construction: 0
2025-04-13 06:35:42 INFO  DatanodeAdminDefaultMonitor:126 - Initialized the Default Decommission and Maintenance monitor
2025-04-13 06:35:42 INFO  BlockManager:5296 - Start MarkedDeleteBlockScrubber thread
2025-04-13 06:35:42 INFO  StateChange:634 - STATE* Safe mode ON. 
The reported blocks 0 needs additional 29 blocks to reach the threshold 0.9990 of total blocks 30.
The minimum number of live datanodes is not required. Safe mode will be turned off automatically once the thresholds have been reached.
2025-04-13 06:35:42 INFO  Server:1713 - IPC Server Responder: starting
2025-04-13 06:35:42 INFO  Server:1553 - IPC Server listener on 9000: starting
2025-04-13 06:35:42 INFO  NameNode:1031 - NameNode RPC up at: hadoop-namenode/172.19.0.5:9000.
2025-04-13 06:35:42 INFO  FSNamesystem:1392 - Starting services required for active state
2025-04-13 06:35:42 INFO  FSDirectory:857 - Initializing quota with 12 thread(s)
2025-04-13 06:35:42 INFO  FSDirectory:866 - Quota initialization completed in 40 milliseconds
name space=32
storage space=6453
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0, NVDIMM=0
2025-04-13 06:35:43 INFO  CacheReplicationMonitor:165 - Starting CacheReplicationMonitor with interval 30000 milliseconds
2025-04-13 06:35:45 INFO  StateChange:1188 - BLOCK* registerDatanode: from DatanodeRegistration(172.19.0.2:9972, datanodeUuid=11da33f8-8e91-48e2-9cf3-9a0d13996193, infoPort=9866, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-ff5816d9-e213-4463-b4f6-bac2bec62c1b;nsid=117484653;c=1744523680573) storage 11da33f8-8e91-48e2-9cf3-9a0d13996193
2025-04-13 06:35:45 INFO  NetworkTopology:156 - Adding a new node: /default-rack/172.19.0.2:9972
2025-04-13 06:35:45 DEBUG NetworkTopology:1132 - Current numOfEmptyRacks is 0
2025-04-13 06:35:45 DEBUG NetworkTopology:165 - NetworkTopology became:
Number of racks: 1
Expected number of leaves:1
/default-rack/172.19.0.2:9972

2025-04-13 06:35:45 INFO  BlockReportLeaseManager:200 - Registered DN 11da33f8-8e91-48e2-9cf3-9a0d13996193 (172.19.0.2:9972).
2025-04-13 06:35:45 INFO  StateChange:1188 - BLOCK* registerDatanode: from DatanodeRegistration(172.19.0.4:9970, datanodeUuid=39a6a668-39fb-4c52-9d8a-8b7f0a867bba, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-ff5816d9-e213-4463-b4f6-bac2bec62c1b;nsid=117484653;c=1744523680573) storage 39a6a668-39fb-4c52-9d8a-8b7f0a867bba
2025-04-13 06:35:45 INFO  NetworkTopology:156 - Adding a new node: /default-rack/172.19.0.4:9970
2025-04-13 06:35:45 DEBUG NetworkTopology:1132 - Current numOfEmptyRacks is 0
2025-04-13 06:35:45 DEBUG NetworkTopology:165 - NetworkTopology became:
Number of racks: 1
Expected number of leaves:2
/default-rack/172.19.0.2:9972
/default-rack/172.19.0.4:9970

2025-04-13 06:35:45 INFO  BlockReportLeaseManager:200 - Registered DN 39a6a668-39fb-4c52-9d8a-8b7f0a867bba (172.19.0.4:9970).
2025-04-13 06:35:46 INFO  DatanodeDescriptor:1065 - Adding new storage ID DS-74b66c21-6b61-4107-a7bb-452ff5841ec0 for DN 172.19.0.2:9972
2025-04-13 06:35:46 INFO  DatanodeDescriptor:1065 - Adding new storage ID DS-2f905a4a-1ad5-4ad3-a48c-b01b7fc6dab4 for DN 172.19.0.4:9970
2025-04-13 06:35:46 INFO  StateChange:1188 - BLOCK* registerDatanode: from DatanodeRegistration(172.19.0.3:9971, datanodeUuid=fa0d5a95-193a-4927-84cf-fd8f2267ab44, infoPort=9865, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-ff5816d9-e213-4463-b4f6-bac2bec62c1b;nsid=117484653;c=1744523680573) storage fa0d5a95-193a-4927-84cf-fd8f2267ab44
2025-04-13 06:35:46 INFO  NetworkTopology:156 - Adding a new node: /default-rack/172.19.0.3:9971
2025-04-13 06:35:46 DEBUG NetworkTopology:1132 - Current numOfEmptyRacks is 0
2025-04-13 06:35:46 DEBUG NetworkTopology:165 - NetworkTopology became:
Number of racks: 1
Expected number of leaves:3
/default-rack/172.19.0.2:9972
/default-rack/172.19.0.4:9970
/default-rack/172.19.0.3:9971

2025-04-13 06:35:46 INFO  BlockReportLeaseManager:200 - Registered DN fa0d5a95-193a-4927-84cf-fd8f2267ab44 (172.19.0.3:9971).
2025-04-13 06:35:46 INFO  BlockStateChange:2940 - BLOCK* processReport 0x4f5bc8d261467729 with lease ID 0x4e0f5ecb2b447602: Processing first storage report for DS-2f905a4a-1ad5-4ad3-a48c-b01b7fc6dab4 from datanode DatanodeRegistration(172.19.0.4:9970, datanodeUuid=39a6a668-39fb-4c52-9d8a-8b7f0a867bba, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-ff5816d9-e213-4463-b4f6-bac2bec62c1b;nsid=117484653;c=1744523680573)
2025-04-13 06:35:46 INFO  BlockManager:5505 - initializing replication queues
2025-04-13 06:35:46 INFO  StateChange:634 - STATE* Safe mode extension entered. 
The reported blocks 29 has reached the threshold 0.9990 of total blocks 30. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2025-04-13 06:35:46 INFO  BlockStateChange:2972 - BLOCK* processReport 0x4f5bc8d261467729 with lease ID 0x4e0f5ecb2b447602: from storage DS-2f905a4a-1ad5-4ad3-a48c-b01b7fc6dab4 node DatanodeRegistration(172.19.0.4:9970, datanodeUuid=39a6a668-39fb-4c52-9d8a-8b7f0a867bba, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-ff5816d9-e213-4463-b4f6-bac2bec62c1b;nsid=117484653;c=1744523680573), blocks: 30, hasStaleStorage: false, processing time: 47 msecs, invalidatedBlocks: 0
2025-04-13 06:35:46 INFO  BlockManager:4030 - Total number of blocks            = 30
2025-04-13 06:35:46 INFO  BlockManager:4031 - Number of invalid blocks          = 0
2025-04-13 06:35:46 INFO  BlockManager:4032 - Number of under-replicated blocks = 29
2025-04-13 06:35:46 INFO  BlockManager:4033 - Number of  over-replicated blocks = 0
2025-04-13 06:35:46 INFO  BlockManager:4035 - Number of blocks being written    = 0
2025-04-13 06:35:46 INFO  StateChange:4038 - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 43 msec
2025-04-13 06:35:46 INFO  BlockManager:4047 - Reconstruction queues initialisation progress: 1.0, total number of blocks processed: 30/30
2025-04-13 06:35:46 INFO  BlockStateChange:2940 - BLOCK* processReport 0x2e4530e817e37fcb with lease ID 0x4e0f5ecb2b447601: Processing first storage report for DS-74b66c21-6b61-4107-a7bb-452ff5841ec0 from datanode DatanodeRegistration(172.19.0.2:9972, datanodeUuid=11da33f8-8e91-48e2-9cf3-9a0d13996193, infoPort=9866, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-ff5816d9-e213-4463-b4f6-bac2bec62c1b;nsid=117484653;c=1744523680573)
2025-04-13 06:35:46 INFO  BlockStateChange:2972 - BLOCK* processReport 0x2e4530e817e37fcb with lease ID 0x4e0f5ecb2b447601: from storage DS-74b66c21-6b61-4107-a7bb-452ff5841ec0 node DatanodeRegistration(172.19.0.2:9972, datanodeUuid=11da33f8-8e91-48e2-9cf3-9a0d13996193, infoPort=9866, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-ff5816d9-e213-4463-b4f6-bac2bec62c1b;nsid=117484653;c=1744523680573), blocks: 30, hasStaleStorage: false, processing time: 14 msecs, invalidatedBlocks: 0
2025-04-13 06:35:46 INFO  DatanodeDescriptor:1065 - Adding new storage ID DS-3ee6931e-8c5a-460e-b62d-eeb5a7f672d8 for DN 172.19.0.3:9971
2025-04-13 06:35:46 INFO  BlockStateChange:2940 - BLOCK* processReport 0x5a21fcd77da9f121 with lease ID 0x4e0f5ecb2b447603: Processing first storage report for DS-3ee6931e-8c5a-460e-b62d-eeb5a7f672d8 from datanode DatanodeRegistration(172.19.0.3:9971, datanodeUuid=fa0d5a95-193a-4927-84cf-fd8f2267ab44, infoPort=9865, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-ff5816d9-e213-4463-b4f6-bac2bec62c1b;nsid=117484653;c=1744523680573)
2025-04-13 06:35:46 INFO  BlockStateChange:2972 - BLOCK* processReport 0x5a21fcd77da9f121 with lease ID 0x4e0f5ecb2b447603: from storage DS-3ee6931e-8c5a-460e-b62d-eeb5a7f672d8 node DatanodeRegistration(172.19.0.3:9971, datanodeUuid=fa0d5a95-193a-4927-84cf-fd8f2267ab44, infoPort=9865, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-ff5816d9-e213-4463-b4f6-bac2bec62c1b;nsid=117484653;c=1744523680573), blocks: 30, hasStaleStorage: false, processing time: 15 msecs, invalidatedBlocks: 0
2025-04-13 06:36:06 INFO  StateChange:634 - STATE* Safe mode ON, in safe mode extension. 
The reported blocks 30 has reached the threshold 0.9990 of total blocks 30. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2025-04-13 06:36:16 INFO  StateChange:404 - STATE* Safe mode is OFF
2025-04-13 06:36:16 INFO  StateChange:409 - STATE* Leaving safe mode after 33 secs
2025-04-13 06:36:16 INFO  StateChange:415 - STATE* Network topology has 1 racks and 3 datanodes
2025-04-13 06:36:16 INFO  StateChange:417 - STATE* UnderReplicatedBlocks has 0 blocks
2025-04-13 06:37:05 INFO  Interns:50 - Metrics intern cache overflow at 2011 for MetricsSystem={MetricsSystem=MetricsInfoImpl{name=MetricsSystem, description=MetricsSystem}, MetricsSystem record=MetricsInfoImpl{name=MetricsSystem, description=MetricsSystem record}}
2025-04-13 06:37:05 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_2035dabe-c556-4f62-a862-38a9f1442f74	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:37:05 INFO  FSEditLog:801 - Number of transactions: 2 Total time for transactions(ms): 11 Number of transactions batched in Syncs: 152 Number of syncs: 2 SyncTimes(ms): 18 
2025-04-13 06:37:05 DEBUG NetworkTopology:542 - Choosing random from 3 available nodes on node /default-rack, scope=/default-rack, excludedScope=null, excludeNodes=[]. numOfDatanodes=3.
2025-04-13 06:37:05 DEBUG NetworkTopology:552 - chooseRandom returning 172.19.0.3:9971
2025-04-13 06:37:05 DEBUG NetworkTopology:525 - Failed to find datanode (scope="" excludedScope="/default-rack"). numOfDatanodes=0
2025-04-13 06:37:05 DEBUG NetworkTopology:125 - No node to choose.
2025-04-13 06:37:05 DEBUG BlockPlacementPolicy:917 - [
  Datanode None is not chosen since required storage types are unavailable  for storage type DISK.
2025-04-13 06:37:05 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:37:05 DEBUG BlockPlacementPolicy:793 - Failed to choose remote rack (location = ~/default-rack), fallback to local rack
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException: 
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:927)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack(BlockPlacementPolicyDefault.java:789)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder(BlockPlacementPolicyDefault.java:569)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:490)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:362)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:182)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:207)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2465)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:293)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3075)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:932)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:603)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1246)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1169)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3198)
2025-04-13 06:37:05 DEBUG NetworkTopology:542 - Choosing random from 2 available nodes on node /default-rack, scope=/default-rack, excludedScope=null, excludeNodes=[172.19.0.3:9971]. numOfDatanodes=3.
2025-04-13 06:37:05 DEBUG NetworkTopology:604 - nthValidToReturn is 1
2025-04-13 06:37:05 DEBUG NetworkTopology:609 - Chosen node 172.19.0.2:9972 from first random
2025-04-13 06:37:05 DEBUG NetworkTopology:552 - chooseRandom returning 172.19.0.2:9972
2025-04-13 06:37:05 DEBUG NetworkTopology:525 - Failed to find datanode (scope="" excludedScope="/default-rack"). numOfDatanodes=0
2025-04-13 06:37:05 DEBUG NetworkTopology:125 - No node to choose.
2025-04-13 06:37:05 DEBUG BlockPlacementPolicy:917 - [
  Datanode None is not chosen since required storage types are unavailable  for storage type DISK.
2025-04-13 06:37:05 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:37:05 DEBUG BlockPlacementPolicy:793 - Failed to choose remote rack (location = ~/default-rack), fallback to local rack
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException: 
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:927)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack(BlockPlacementPolicyDefault.java:789)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder(BlockPlacementPolicyDefault.java:578)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:490)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:362)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:182)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:207)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2465)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:293)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3075)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:932)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:603)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1246)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1169)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3198)
2025-04-13 06:37:05 DEBUG NetworkTopology:542 - Choosing random from 1 available nodes on node /default-rack, scope=/default-rack, excludedScope=null, excludeNodes=[172.19.0.3:9971, 172.19.0.2:9972]. numOfDatanodes=3.
2025-04-13 06:37:05 DEBUG NetworkTopology:604 - nthValidToReturn is 0
2025-04-13 06:37:05 DEBUG NetworkTopology:609 - Chosen node 172.19.0.4:9970 from first random
2025-04-13 06:37:05 DEBUG NetworkTopology:552 - chooseRandom returning 172.19.0.4:9970
2025-04-13 06:37:05 INFO  StateChange:801 - BLOCK* allocate blk_1073741855_1031, replicas=172.19.0.3:9971, 172.19.0.2:9972, 172.19.0.4:9970 for /data/message_2035dabe-c556-4f62-a862-38a9f1442f74
2025-04-13 06:37:05 INFO  FSNamesystem:3286 - BLOCK* blk_1073741855_1031 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /data/message_2035dabe-c556-4f62-a862-38a9f1442f74
2025-04-13 06:37:06 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741855_1031 (size=72)
2025-04-13 06:37:06 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.2:9972 is added to blk_1073741855_1031 (size=72)
2025-04-13 06:37:06 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741855_1031 (size=72)
2025-04-13 06:37:06 INFO  StateChange:3239 - DIR* completeFile: /data/message_2035dabe-c556-4f62-a862-38a9f1442f74 is closed by DFSClient_NONMAPREDUCE_1879161365_1
2025-04-13 06:37:06 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_22e1b117-8f86-44c9-af79-8c9bf381efb4	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:37:06 DEBUG NetworkTopology:542 - Choosing random from 3 available nodes on node /default-rack, scope=/default-rack, excludedScope=null, excludeNodes=[]. numOfDatanodes=3.
2025-04-13 06:37:06 DEBUG NetworkTopology:552 - chooseRandom returning 172.19.0.4:9970
2025-04-13 06:37:06 DEBUG NetworkTopology:525 - Failed to find datanode (scope="" excludedScope="/default-rack"). numOfDatanodes=0
2025-04-13 06:37:06 DEBUG NetworkTopology:125 - No node to choose.
2025-04-13 06:37:06 DEBUG BlockPlacementPolicy:917 - [
  Datanode None is not chosen since required storage types are unavailable  for storage type DISK.
2025-04-13 06:37:06 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:37:06 DEBUG BlockPlacementPolicy:793 - Failed to choose remote rack (location = ~/default-rack), fallback to local rack
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException: 
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:927)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack(BlockPlacementPolicyDefault.java:789)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder(BlockPlacementPolicyDefault.java:569)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:490)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:362)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:182)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:207)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2465)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:293)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3075)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:932)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:603)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1246)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1169)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3198)
2025-04-13 06:37:06 DEBUG NetworkTopology:542 - Choosing random from 2 available nodes on node /default-rack, scope=/default-rack, excludedScope=null, excludeNodes=[172.19.0.4:9970]. numOfDatanodes=3.
2025-04-13 06:37:06 DEBUG NetworkTopology:604 - nthValidToReturn is 0
2025-04-13 06:37:06 DEBUG NetworkTopology:609 - Chosen node 172.19.0.2:9972 from first random
2025-04-13 06:37:06 DEBUG NetworkTopology:552 - chooseRandom returning 172.19.0.2:9972
2025-04-13 06:37:06 DEBUG NetworkTopology:525 - Failed to find datanode (scope="" excludedScope="/default-rack"). numOfDatanodes=0
2025-04-13 06:37:06 DEBUG NetworkTopology:125 - No node to choose.
2025-04-13 06:37:06 DEBUG BlockPlacementPolicy:917 - [
  Datanode None is not chosen since required storage types are unavailable  for storage type DISK.
2025-04-13 06:37:06 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:37:06 DEBUG BlockPlacementPolicy:793 - Failed to choose remote rack (location = ~/default-rack), fallback to local rack
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException: 
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:927)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack(BlockPlacementPolicyDefault.java:789)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder(BlockPlacementPolicyDefault.java:578)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:490)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:362)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:182)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:207)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2465)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:293)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3075)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:932)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:603)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1246)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1169)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3198)
2025-04-13 06:37:06 DEBUG NetworkTopology:542 - Choosing random from 1 available nodes on node /default-rack, scope=/default-rack, excludedScope=null, excludeNodes=[172.19.0.4:9970, 172.19.0.2:9972]. numOfDatanodes=3.
2025-04-13 06:37:06 DEBUG NetworkTopology:604 - nthValidToReturn is 0
2025-04-13 06:37:06 DEBUG NetworkTopology:624 - Node 172.19.0.2:9972 is excluded, continuing.
2025-04-13 06:37:06 DEBUG NetworkTopology:624 - Node 172.19.0.4:9970 is excluded, continuing.
2025-04-13 06:37:06 DEBUG NetworkTopology:552 - chooseRandom returning 172.19.0.3:9971
2025-04-13 06:37:06 INFO  StateChange:801 - BLOCK* allocate blk_1073741856_1032, replicas=172.19.0.4:9970, 172.19.0.2:9972, 172.19.0.3:9971 for /data/message_22e1b117-8f86-44c9-af79-8c9bf381efb4
2025-04-13 06:37:06 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741856_1032 (size=72)
2025-04-13 06:37:06 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.2:9972 is added to blk_1073741856_1032 (size=72)
2025-04-13 06:37:06 INFO  StateChange:3239 - DIR* completeFile: /data/message_22e1b117-8f86-44c9-af79-8c9bf381efb4 is closed by DFSClient_NONMAPREDUCE_1879161365_1
2025-04-13 06:37:06 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741856_1032 (size=72)
2025-04-13 06:37:06 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_53a2be62-cc4c-4b80-b178-444181443ca4	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:37:06 DEBUG NetworkTopology:542 - Choosing random from 3 available nodes on node /default-rack, scope=/default-rack, excludedScope=null, excludeNodes=[]. numOfDatanodes=3.
2025-04-13 06:37:06 DEBUG NetworkTopology:552 - chooseRandom returning 172.19.0.2:9972
2025-04-13 06:37:06 DEBUG NetworkTopology:525 - Failed to find datanode (scope="" excludedScope="/default-rack"). numOfDatanodes=0
2025-04-13 06:37:06 DEBUG NetworkTopology:125 - No node to choose.
2025-04-13 06:37:06 DEBUG BlockPlacementPolicy:917 - [
  Datanode None is not chosen since required storage types are unavailable  for storage type DISK.
2025-04-13 06:37:06 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:37:06 DEBUG BlockPlacementPolicy:793 - Failed to choose remote rack (location = ~/default-rack), fallback to local rack
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException: 
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:927)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack(BlockPlacementPolicyDefault.java:789)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder(BlockPlacementPolicyDefault.java:569)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:490)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:362)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:182)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:207)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2465)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:293)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3075)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:932)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:603)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1246)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1169)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3198)
2025-04-13 06:37:06 DEBUG NetworkTopology:542 - Choosing random from 2 available nodes on node /default-rack, scope=/default-rack, excludedScope=null, excludeNodes=[172.19.0.2:9972]. numOfDatanodes=3.
2025-04-13 06:37:06 DEBUG NetworkTopology:604 - nthValidToReturn is 0
2025-04-13 06:37:06 DEBUG NetworkTopology:624 - Node 172.19.0.2:9972 is excluded, continuing.
2025-04-13 06:37:06 DEBUG NetworkTopology:552 - chooseRandom returning 172.19.0.4:9970
2025-04-13 06:37:06 DEBUG NetworkTopology:525 - Failed to find datanode (scope="" excludedScope="/default-rack"). numOfDatanodes=0
2025-04-13 06:37:06 DEBUG NetworkTopology:125 - No node to choose.
2025-04-13 06:37:06 DEBUG BlockPlacementPolicy:917 - [
  Datanode None is not chosen since required storage types are unavailable  for storage type DISK.
2025-04-13 06:37:06 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:37:06 DEBUG BlockPlacementPolicy:793 - Failed to choose remote rack (location = ~/default-rack), fallback to local rack
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException: 
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:927)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack(BlockPlacementPolicyDefault.java:789)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder(BlockPlacementPolicyDefault.java:578)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:490)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:362)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:182)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:207)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2465)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:293)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3075)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:932)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:603)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1246)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1169)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3198)
2025-04-13 06:37:06 DEBUG NetworkTopology:542 - Choosing random from 1 available nodes on node /default-rack, scope=/default-rack, excludedScope=null, excludeNodes=[172.19.0.4:9970, 172.19.0.2:9972]. numOfDatanodes=3.
2025-04-13 06:37:06 DEBUG NetworkTopology:604 - nthValidToReturn is 0
2025-04-13 06:37:06 DEBUG NetworkTopology:624 - Node 172.19.0.2:9972 is excluded, continuing.
2025-04-13 06:37:06 DEBUG NetworkTopology:624 - Node 172.19.0.4:9970 is excluded, continuing.
2025-04-13 06:37:06 DEBUG NetworkTopology:552 - chooseRandom returning 172.19.0.3:9971
2025-04-13 06:37:06 INFO  StateChange:801 - BLOCK* allocate blk_1073741857_1033, replicas=172.19.0.2:9972, 172.19.0.4:9970, 172.19.0.3:9971 for /data/message_53a2be62-cc4c-4b80-b178-444181443ca4
2025-04-13 06:37:06 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741857_1033 (size=72)
2025-04-13 06:37:06 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741857_1033 (size=72)
2025-04-13 06:37:06 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.2:9972 is added to blk_1073741857_1033 (size=72)
2025-04-13 06:37:06 INFO  StateChange:3239 - DIR* completeFile: /data/message_53a2be62-cc4c-4b80-b178-444181443ca4 is closed by DFSClient_NONMAPREDUCE_1879161365_1
2025-04-13 06:37:07 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_3061aaf4-21fa-489c-9fc2-dca58f5e6a94	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:37:07 DEBUG NetworkTopology:542 - Choosing random from 3 available nodes on node /default-rack, scope=/default-rack, excludedScope=null, excludeNodes=[]. numOfDatanodes=3.
2025-04-13 06:37:07 DEBUG NetworkTopology:552 - chooseRandom returning 172.19.0.3:9971
2025-04-13 06:37:07 DEBUG NetworkTopology:525 - Failed to find datanode (scope="" excludedScope="/default-rack"). numOfDatanodes=0
2025-04-13 06:37:07 DEBUG NetworkTopology:125 - No node to choose.
2025-04-13 06:37:07 DEBUG BlockPlacementPolicy:917 - [
  Datanode None is not chosen since required storage types are unavailable  for storage type DISK.
2025-04-13 06:37:07 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:37:07 DEBUG BlockPlacementPolicy:793 - Failed to choose remote rack (location = ~/default-rack), fallback to local rack
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException: 
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:927)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack(BlockPlacementPolicyDefault.java:789)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder(BlockPlacementPolicyDefault.java:569)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:490)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:362)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:182)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:207)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2465)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:293)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3075)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:932)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:603)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1246)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1169)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3198)
2025-04-13 06:37:07 DEBUG NetworkTopology:542 - Choosing random from 2 available nodes on node /default-rack, scope=/default-rack, excludedScope=null, excludeNodes=[172.19.0.3:9971]. numOfDatanodes=3.
2025-04-13 06:37:07 DEBUG NetworkTopology:604 - nthValidToReturn is 0
2025-04-13 06:37:07 DEBUG NetworkTopology:552 - chooseRandom returning 172.19.0.2:9972
2025-04-13 06:37:07 DEBUG NetworkTopology:525 - Failed to find datanode (scope="" excludedScope="/default-rack"). numOfDatanodes=0
2025-04-13 06:37:07 DEBUG NetworkTopology:125 - No node to choose.
2025-04-13 06:37:07 DEBUG BlockPlacementPolicy:917 - [
  Datanode None is not chosen since required storage types are unavailable  for storage type DISK.
2025-04-13 06:37:07 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:37:07 DEBUG BlockPlacementPolicy:793 - Failed to choose remote rack (location = ~/default-rack), fallback to local rack
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException: 
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:927)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack(BlockPlacementPolicyDefault.java:789)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder(BlockPlacementPolicyDefault.java:578)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:490)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:362)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:182)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:207)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2465)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:293)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3075)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:932)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:603)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1246)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1169)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3198)
2025-04-13 06:37:07 DEBUG NetworkTopology:542 - Choosing random from 1 available nodes on node /default-rack, scope=/default-rack, excludedScope=null, excludeNodes=[172.19.0.3:9971, 172.19.0.2:9972]. numOfDatanodes=3.
2025-04-13 06:37:07 DEBUG NetworkTopology:604 - nthValidToReturn is 0
2025-04-13 06:37:07 DEBUG NetworkTopology:609 - Chosen node 172.19.0.4:9970 from first random
2025-04-13 06:37:07 DEBUG NetworkTopology:552 - chooseRandom returning 172.19.0.4:9970
2025-04-13 06:37:07 INFO  StateChange:801 - BLOCK* allocate blk_1073741858_1034, replicas=172.19.0.3:9971, 172.19.0.2:9972, 172.19.0.4:9970 for /data/message_3061aaf4-21fa-489c-9fc2-dca58f5e6a94
2025-04-13 06:37:07 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741858_1034 (size=69)
2025-04-13 06:37:07 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.2:9972 is added to blk_1073741858_1034 (size=69)
2025-04-13 06:37:08 INFO  StateChange:3239 - DIR* completeFile: /data/message_3061aaf4-21fa-489c-9fc2-dca58f5e6a94 is closed by DFSClient_NONMAPREDUCE_1879161365_1
2025-04-13 06:37:08 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741858_1034 (size=69)
2025-04-13 06:37:10 INFO  audit:8947 - allowed=true	ugi=hadoop (auth:SIMPLE)	ip=/192.168.56.1	cmd=create	src=/data/message_2de40cfe-a365-49f2-9b28-6697c2eca224	dst=null	perm=hadoop:supergroup:rw-r--r--	proto=rpc
2025-04-13 06:37:10 DEBUG NetworkTopology:542 - Choosing random from 3 available nodes on node /default-rack, scope=/default-rack, excludedScope=null, excludeNodes=[]. numOfDatanodes=3.
2025-04-13 06:37:10 DEBUG NetworkTopology:552 - chooseRandom returning 172.19.0.3:9971
2025-04-13 06:37:10 DEBUG NetworkTopology:525 - Failed to find datanode (scope="" excludedScope="/default-rack"). numOfDatanodes=0
2025-04-13 06:37:10 DEBUG NetworkTopology:125 - No node to choose.
2025-04-13 06:37:10 DEBUG BlockPlacementPolicy:917 - [
  Datanode None is not chosen since required storage types are unavailable  for storage type DISK.
2025-04-13 06:37:10 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:37:10 DEBUG BlockPlacementPolicy:793 - Failed to choose remote rack (location = ~/default-rack), fallback to local rack
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException: 
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:927)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack(BlockPlacementPolicyDefault.java:789)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder(BlockPlacementPolicyDefault.java:569)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:490)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:362)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:182)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:207)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2465)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:293)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3075)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:932)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:603)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1246)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1169)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3198)
2025-04-13 06:37:10 DEBUG NetworkTopology:542 - Choosing random from 2 available nodes on node /default-rack, scope=/default-rack, excludedScope=null, excludeNodes=[172.19.0.3:9971]. numOfDatanodes=3.
2025-04-13 06:37:10 DEBUG NetworkTopology:604 - nthValidToReturn is 0
2025-04-13 06:37:10 DEBUG NetworkTopology:609 - Chosen node 172.19.0.4:9970 from first random
2025-04-13 06:37:10 DEBUG NetworkTopology:552 - chooseRandom returning 172.19.0.4:9970
2025-04-13 06:37:10 DEBUG NetworkTopology:525 - Failed to find datanode (scope="" excludedScope="/default-rack"). numOfDatanodes=0
2025-04-13 06:37:10 DEBUG NetworkTopology:125 - No node to choose.
2025-04-13 06:37:10 DEBUG BlockPlacementPolicy:917 - [
  Datanode None is not chosen since required storage types are unavailable  for storage type DISK.
2025-04-13 06:37:10 INFO  BlockPlacementPolicy:925 - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2025-04-13 06:37:10 DEBUG BlockPlacementPolicy:793 - Failed to choose remote rack (location = ~/default-rack), fallback to local rack
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException: 
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:927)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack(BlockPlacementPolicyDefault.java:789)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder(BlockPlacementPolicyDefault.java:578)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:490)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:362)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:182)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:207)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2465)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:293)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3075)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:932)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:603)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1246)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1169)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3198)
2025-04-13 06:37:10 DEBUG NetworkTopology:542 - Choosing random from 1 available nodes on node /default-rack, scope=/default-rack, excludedScope=null, excludeNodes=[172.19.0.3:9971, 172.19.0.4:9970]. numOfDatanodes=3.
2025-04-13 06:37:10 DEBUG NetworkTopology:604 - nthValidToReturn is 0
2025-04-13 06:37:10 DEBUG NetworkTopology:552 - chooseRandom returning 172.19.0.2:9972
2025-04-13 06:37:10 INFO  StateChange:801 - BLOCK* allocate blk_1073741859_1035, replicas=172.19.0.3:9971, 172.19.0.4:9970, 172.19.0.2:9972 for /data/message_2de40cfe-a365-49f2-9b28-6697c2eca224
2025-04-13 06:37:11 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.2:9972 is added to blk_1073741859_1035 (size=72)
2025-04-13 06:37:11 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.4:9970 is added to blk_1073741859_1035 (size=72)
2025-04-13 06:37:11 INFO  BlockStateChange:3777 - BLOCK* addStoredBlock: 172.19.0.3:9971 is added to blk_1073741859_1035 (size=72)
2025-04-13 06:37:11 INFO  StateChange:3239 - DIR* completeFile: /data/message_2de40cfe-a365-49f2-9b28-6697c2eca224 is closed by DFSClient_NONMAPREDUCE_1879161365_1
2025-04-13 06:59:47 INFO  JvmPauseMonitor:202 - Detected pause in JVM or host machine (eg GC): pause of approximately 6358ms
No GCs detected
